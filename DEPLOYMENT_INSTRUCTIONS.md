# Deployment Instructions - Context Filtering Fixes

## Changes Made

### 1. Payment Context Filtering ‚úÖ
- Added "—Å–ø–ª–∞—Ç–∞" to all payment lexicons
- **Files modified**: `payment_context.txt`, `stopwords.py`, `lexicon.py`

### 2. Insurance Context Filtering ‚úÖ
- Added comprehensive insurance terms: —Å—Ç—Ä–∞—Ö–æ–≤–∏–π, –ø–æ–ª—ñ—Å, –û–°–¶–ü–í, –ö–ê–°–ö–û
- **Files modified**: `payment_context.txt`, `stopwords.py`, `lexicon.py`

### 3. Transport Context Filtering ‚úÖ
- Added transport terms: –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è, —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∞, –∫–∞—Ä—Ç–∏, –ø—Ä–æ—ó–∑–¥, etc.
- **Files modified**: `payment_context.txt`, `stopwords.py`, `lexicon.py`

## Current Status

‚úÖ **Code pushed** to remote repository
‚ö†Ô∏è **Server restart required** - changes not yet applied
üîç **Feature flags working** - factory implementation correctly chosen

## Required Steps

### 1. Restart Production Server

The changes are in the code but require server restart to load new lexicons:

```bash
# SSH to production server
ssh production-server

# Navigate to service directory
cd /path/to/ai-service

# Pull latest changes
git pull origin main

# Restart the service (choose appropriate method)
# Option A: Docker restart
docker-compose restart ai-service

# Option B: Systemd restart
sudo systemctl restart ai-service

# Option C: Manual restart
pkill -f "python.*main.py"
python src/ai_service/main.py &
```

### 2. Verify Deployment

After restart, test with the transport example:

```bash
curl -X POST 'http://95.217.84.234:8000/process' \
-H 'Content-Type: application/json' \
-d '{
  "text": "–ü–æ–ø–æ–≤–Ω–µ–Ω–Ω—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ—ó –∫–∞—Ä—Ç–∏ 1000",
  "options": {
    "flags": {
      "use_factory_normalizer": true,
      "normalization_implementation": "factory"
    }
  }
}'
```

**Expected result after restart:**
```json
{
  "normalized_text": "",  // Empty - transport terms filtered
  "tokens": [],
  "signals": {
    "organizations": [],  // No false person entities
    "persons": []
  }
}
```

**Current result (before restart):**
```json
{
  "normalized_text": "–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∞ –ü–æ–ø–æ–≤–Ω–µ–Ω–Ω—è",  // ‚ùå Wrong
  "tokens": ["–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∞", "–ü–æ–ø–æ–≤–Ω–µ–Ω–Ω—è"],
  "signals": {
    "persons": [{"core": ["–ü–æ–ø–æ–≤–Ω–µ–Ω–Ω—è", "—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∞"]}]  // ‚ùå False person
  }
}
```

### 3. Test Cases for Verification

After restart, test all context types:

```bash
# Test 1: Payment context
curl -X POST 'http://95.217.84.234:8000/process' -d '{"text": "–°–ø–ª–∞—Ç–∞ –∑–∞ –ø–æ—Å–ª—É–≥–∏ 100 –≥—Ä–Ω"}'
# Expected: Payment terms filtered, no false names

# Test 2: Insurance context
curl -X POST 'http://95.217.84.234:8000/process' -d '{"text": "–°—Ç—Ä–∞—Ö–æ–≤–∏–π –ø–ª–∞—Ç—ñ–∂ –∑–∞ –ø–æ–ª—ñ—Å –û–°–¶–ü–í"}'
# Expected: Insurance terms filtered, no false names

# Test 3: Transport context
curl -X POST 'http://95.217.84.234:8000/process' -d '{"text": "–ü–æ–ø–æ–≤–Ω–µ–Ω–Ω—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ—ó –∫–∞—Ä—Ç–∏ –º–µ—Ç—Ä–æ"}'
# Expected: Transport terms filtered, no false names

# Test 4: Real names (should still work)
curl -X POST 'http://95.217.84.234:8000/process' -d '{"text": "–•–∞–º—ñ–Ω –í–ª–∞–¥–∏—Å–ª–∞–≤ –Ü–≥–æ—Ä–æ–≤–∏—á"}'
# Expected: Names correctly extracted and normalized
```

## Rollback Plan

If issues occur, rollback to previous version:

```bash
# Revert to previous commit
git reset --hard HEAD~3

# Restart service
docker-compose restart ai-service
```

## Monitoring

Check server logs for:
- ‚úÖ `"Using factory implementation for language=uk"` (factory used)
- ‚úÖ `"Filtered stop word: '–ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è'"` (transport terms filtered)
- ‚úÖ `"Filtered stop word: '—Å–ø–ª–∞—Ç–∞'"` (payment terms filtered)
- ‚ùå Any new errors or performance issues

## Files Changed

```
data/lexicons/payment_context.txt          # Added 50+ new terms
src/ai_service/data/dicts/stopwords.py     # Added to STOP_ALL
src/ai_service/layers/variants/templates/lexicon.py  # Added to all language sets
```

## Commits Applied

1. `fix(normalization): add "—Å–ø–ª–∞—Ç–∞" to payment context filters`
2. `fix(normalization): add insurance context filtering`
3. `fix(normalization): add transport context filtering`

---

**Next Action Required**: Server restart to apply lexicon changes.