#!/usr/bin/env python3
"""
Debug search issues
"""

import sys
sys.path.insert(0, '.')
sys.path.insert(0, 'src')

def check_homoglyph_detection():
    """Check homoglyph detection for mixed script names."""
    print("üîç HOMOGLYPH DETECTION TEST")
    print("="*40)

    test_name = "Liud–º—Él–∞ Uli–∞n–æv–∞"  # Mixed Latin/Cyrillic

    print(f"Testing: '{test_name}'")

    # Check each character
    for i, char in enumerate(test_name):
        ord_val = ord(char)
        is_latin = ord_val <= 127
        is_cyrillic = 0x0400 <= ord_val <= 0x04FF

        if char != ' ':
            print(f"  [{i}] '{char}' -> ord={ord_val} {'(Latin)' if is_latin else '(Cyrillic)' if is_cyrillic else '(Other)'}")

    # Check ASCII/Unicode mix
    has_ascii = any(ord(c) <= 127 for c in test_name if c != ' ')
    has_unicode = any(ord(c) > 127 for c in test_name if c != ' ')

    print(f"\nüìä Analysis:")
    print(f"  Has ASCII: {has_ascii}")
    print(f"  Has Unicode: {has_unicode}")
    print(f"  Mixed script: {has_ascii and has_unicode}")

    if has_ascii and has_unicode:
        print("  üö® HOMOGLYPH ATTACK DETECTED!")

        # Try to normalize
        normalized_parts = []
        for word in test_name.split():
            # Simple normalization attempt
            normalized = ""
            for char in word:
                if ord(char) <= 127:  # ASCII
                    normalized += char
                elif char in "–º—É–ª":  # Common Cyrillic that might be confused
                    if char == "–º":
                        normalized += "m"
                    elif char == "—É":
                        normalized += "y"
                    elif char == "–ª":
                        normalized += "l"
                    else:
                        normalized += char
                else:
                    normalized += char
            normalized_parts.append(normalized)

        suggested_normal = " ".join(normalized_parts)
        print(f"  üí° Suggested normalization: '{suggested_normal}'")

async def check_sanctions_data():
    """Check what's actually in sanctions data."""
    print(f"\nüîç SANCTIONS DATA CHECK")
    print("="*40)

    try:
        from ai_service.layers.search.sanctions_data_loader import SanctionsDataLoader

        loader = SanctionsDataLoader()
        dataset = await loader.load_dataset(force_reload=False)

        print(f"üìä Dataset stats:")
        print(f"  Total entries: {dataset.total_entries}")
        print(f"  Unique names: {len(dataset.all_names)}")

        # Search for similar names
        search_terms = [
            "–õ—é–¥–º–∏–ª–∞",
            "–£–ª—å—è–Ω–æ–≤–∞",
            "–õ—é–¥–º–∏–ª–∞ –£–ª—å—è–Ω–æ–≤–∞",
            "–®–µ–≤—á–µ–Ω–∫–æ",
            "–ê–Ω–∞—Ç–æ–ª—ñ–π–æ–≤–∏—á",
            "–ê–Ω–∞—Ç–æ–ª–∏–π",
            "Andriy"
        ]

        print(f"\nüîç Searching for similar names:")
        for term in search_terms:
            found_names = [name for name in dataset.all_names if term.lower() in name.lower()]
            if found_names:
                print(f"  '{term}' -> Found {len(found_names)} matches:")
                for name in found_names[:5]:  # Show first 5
                    print(f"    - {name}")
                if len(found_names) > 5:
                    print(f"    ... and {len(found_names) - 5} more")
            else:
                print(f"  '{term}' -> No matches")

    except Exception as e:
        print(f"‚ùå Failed to check sanctions data: {e}")

def check_normalization_issue():
    """Check the strange normalization issue."""
    print(f"\nüîç NORMALIZATION ISSUE")
    print("="*40)

    # This shows what went wrong with the second case
    problem_trace = [
        "–®–ï–í–ß–ï–ù–ö–û -> surname",
        "–ê–ù–î–†–Ü–ô -> unknown",
        "–ê–ù–ê–¢–û–õ–Ü–ô–û–í–ò–ß -> patronymic",
        "–®–µ–≤—á–µ–Ω–∫–æ -> surname (duplicate?)",
        "–ê–Ω–∞—Ç–æ–ª—ñ–π–æ–≤–∏—á -> patronymic (duplicate?)"
    ]

    print("Problem trace shows duplicated tokens:")
    for trace in problem_trace:
        print(f"  {trace}")

    print("\nüí° Issues:")
    print("  1. –ê–ù–î–†–Ü–ô marked as 'unknown' - should be 'given'")
    print("  2. Duplicated –®–µ–≤—á–µ–Ω–∫–æ/–ê–Ω–∞—Ç–æ–ª—ñ–π–æ–≤–∏—á processing")
    print("  3. Mixed case output in signals")

    expected_result = "–®–µ–≤—á–µ–Ω–∫–æ –ê–Ω–¥—Ä—ñ–π –ê–Ω–∞—Ç–æ–ª—ñ–π–æ–≤–∏—á"
    actual_signals = "–®–ï–í–ß–ï–ù–ö–û –∞–Ω–¥—Ä—ñ–π –ê–Ω–∞—Ç–æ–ª—ñ–π–æ–≤–∏—á –®–ï–í–ß–ï–ù–ö–û –ê–Ω–∞—Ç–æ–ª—ñ–π–æ–≤–∏—á"

    print(f"\n  Expected: '{expected_result}'")
    print(f"  Actual signals: '{actual_signals}'")

if __name__ == "__main__":
    import asyncio

    check_homoglyph_detection()
    asyncio.run(check_sanctions_data())
    check_normalization_issue()