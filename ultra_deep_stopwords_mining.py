#!/usr/bin/env python3
"""
–£–ª—å—Ç—Ä–∞-–≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤
–¶–µ–ª—å: –≤—ã–∂–∞—Ç—å –∏–∑ 1–ú —Ç–æ–∫–µ–Ω–æ–≤ –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –º—É—Å–æ—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
"""

import csv
import sys
import re
sys.path.append('/Users/dariapavlova/Desktop/ai-service/src')

from ai_service.data.dicts.stopwords import STOP_ALL

def ultra_deep_mining(filename, top_n=10000):
    """–£–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –≤ —Ç–æ–ø-10–ö —Ç–æ–∫–µ–Ω–æ–≤"""

    print(f"üî• –£–õ–¨–¢–†–ê-–ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó —Ç–æ–ø-{top_n} —Ç–æ–∫–µ–Ω–æ–≤")
    print(f"üìö –¢–µ–∫—É—â–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞: {len(STOP_ALL)}")
    print("üéØ –¶–ï–õ–¨: –ù–∞–π—Ç–∏ –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –º—É—Å–æ—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∏—è")
    print("=" * 80)

    # –ë–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —É–ª—å—Ç—Ä–∞-–∞–Ω–∞–ª–∏–∑–∞
    mining_categories = {
        'numbers_and_codes': [],        # –õ—é–±—ã–µ —á–∏—Å–ª–∞, –∫–æ–¥—ã, –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        'tech_fragments': [],           # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ–±—Ä—ã–≤–∫–∏, –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        'business_noise': [],           # –õ—é–±—ã–µ –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω—ã
        'descriptive_words': [],        # –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞, –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ
        'conjunctions_particles': [],   # –°–æ—é–∑—ã, —á–∞—Å—Ç–∏—Ü—ã, –ø—Ä–µ–¥–ª–æ–≥–∏
        'verb_forms': [],              # –ì–ª–∞–≥–æ–ª—ã –≤ –ª—é–±–æ–π —Ñ–æ—Ä–º–µ
        'administrative': [],          # –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        'service_operations': [],      # –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã —É—Å–ª—É–≥
        'measurement_quantities': [],   # –ò–∑–º–µ—Ä–µ–Ω–∏—è, –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞, –µ–¥–∏–Ω–∏—Ü—ã
        'temporal_references': [],     # –õ—é–±—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        'geographical_misc': [],       # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        'financial_terms': [],         # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        'high_freq_garbage': [],       # –í—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–π –º—É—Å–æ—Ä
        'short_noise': [],             # –ö–æ—Ä–æ—Ç–∫–∏–µ —à—É–º–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
        'suspicious_patterns': []      # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    }

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —É–ª—å—Ç—Ä–∞-–ø–æ–∏—Å–∫–∞
    patterns = {
        'numbers_codes': [
            r'^\d+$',                    # –ß–∏—Å—Ç—ã–µ —á–∏—Å–ª–∞
            r'^[0-9a-f]{6,}$',          # –•–µ–∫—Å-–∫–æ–¥—ã
            r'^[a-z]{1,3}\d+$',         # –ë—É–∫–≤—ã + —á–∏—Å–ª–∞ (–∫–æ–¥1, abc123)
            r'^\d+[a-z]{1,3}$',         # –ß–∏—Å–ª–∞ + –±—É–∫–≤—ã (123abc)
            r'^[0-9-]{6,}$',            # –ß–∏—Å–ª–∞ —Å –¥–µ—Ñ–∏—Å–∞–º–∏
            r'^[0-9\.]{6,}$',           # –ß–∏—Å–ª–∞ —Å —Ç–æ—á–∫–∞–º–∏
        ],

        'fragments': [
            r'^.{1,3}$',                # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ (1-3 —Å–∏–º–≤–æ–ª–∞)
            r'^[–∞-—è]{1,4}[0-9]',        # –ö–æ—Ä–æ—Ç–∫–∏–µ + —Ü–∏—Ñ—Ä–∞
            r'^[a-z]{1,4}[0-9]',        # –õ–∞—Ç–∏–Ω–∏—Ü–∞ + —Ü–∏—Ñ—Ä–∞
            r'[0-9]$',                  # –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Ü–∏—Ñ—Ä–æ–π
        ],

        'business_patterns': [
            '–∫–æ–º–ø–∞–Ω', '—Ñ–∏—Ä–º', '–ø—Ä–µ–¥–ø—Ä–∏—è—Ç', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü', '—É—á—Ä–µ–∂–¥–µ–Ω',
            '–æ–±—â–µ—Å—Ç', '—Ç–æ–≤–∞—Ä–∏—â', '–æ–±—ä–µ–¥–∏–Ω', '—Å–æ—é–∑', '–∞—Å—Å–æ—Ü–∏–∞—Ü',
            '–∫–æ—Ä–ø–æ—Ä–∞—Ü', '—Ö–æ–ª–¥–∏–Ω–≥', '–∫–æ–Ω—Ü–µ—Ä–Ω', '–≥—Ä—É–ø–ø–∞', '–≥—Ä—É–ø'
        ],

        'service_patterns': [
            '—É—Å–ª—É–≥', '—Å–µ—Ä–≤–∏—Å', '–æ–±—Å–ª—É–∂', '–æ–ø–µ—Ä–∞—Ü', '–ø—Ä–æ—Ü–µ–¥—É—Ä', '—Ä–∞–±–æ—Ç',
            '–¥–µ—è—Ç–µ–ª—å–Ω', '—Ñ—É–Ω–∫—Ü', '–ø—Ä–æ—Ü–µ—Å—Å', '—Å–∏—Å—Ç–µ–º', '–º–µ—Ö–∞–Ω–∏–∑–º',
            '–ø–æ—Ä—è–¥–æ–∫', '–ø—Ä–∞–≤–∏–ª', '–Ω–æ—Ä–º', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç', '—Ç—Ä–µ–±–æ–≤–∞–Ω'
        ],

        'admin_patterns': [
            '—É–ø—Ä–∞–≤–ª–µ–Ω', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä', '–∫–æ–Ω—Ç—Ä–æ–ª', '–Ω–∞–¥–∑–æ—Ä', '–∏–Ω—Å–ø–µ–∫—Ü',
            '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤', '–∫–æ–º–∏—Ç–µ—Ç', '–∫–æ–º–∏—Å—Å', '—Å–æ–≤–µ—Ç',
            '–æ—Ä–≥–∞–Ω', '–≤–ª–∞—Å—Ç', '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤', '–º—É–Ω–∏—Ü–∏–ø–∞–ª', '–º–µ—Å—Ç–Ω'
        ],

        'descriptive_patterns': [
            '–±–æ–ª—å—à', '–º–∞–ª', '—Å—Ä–µ–¥–Ω', '–æ–±—â', '—Å–ø–µ—Ü–∏–∞–ª—å–Ω', '–æ—Å–Ω–æ–≤–Ω',
            '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω', '–Ω–æ–≤—ã–π', '—Å—Ç–∞—Ä', '–º–æ–ª–æ–¥', '—Ö–æ—Ä–æ—à', '–ø–ª–æ—Ö',
            '–≤—ã—Å–æ–∫', '–Ω–∏–∑–∫', '—à–∏—Ä–æ–∫', '—É–∑–∫', '–¥–ª–∏–Ω–Ω', '–∫–æ—Ä–æ—Ç–∫'
        ]
    }

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ –ù–ï –∏–º–µ–Ω–∞ - –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    definitely_not_names = [
        # –û—á–µ–≤–∏–¥–Ω—ã–µ –∏–º–µ–Ω–∞ - –ù–ï —Ç—Ä–æ–≥–∞–µ–º
        '–∞–ª–µ–∫—Å–∞–Ω–¥—Ä', '–≤–ª–∞–¥–∏–º–∏—Ä', '—Å–µ—Ä–≥–µ–π', '–∞–Ω–¥—Ä–µ–π', '–¥–º–∏—Ç—Ä–∏–π', '–Ω–∏–∫–æ–ª–∞–π', '–∞–ª–µ–∫—Å–µ–π',
        '–≤–∏–∫—Ç–æ—Ä', '–ø–µ—Ç—Ä', '–∏–≤–∞–Ω', '—é—Ä–∏–π', '–º–∏—Ö–∞–∏–ª', '–µ–≤–≥–µ–Ω–∏–π', '–≤–∞—Å–∏–ª–∏–π', '–∞–Ω–∞—Ç–æ–ª–∏–π',
        '–∞–Ω–Ω–∞', '–º–∞—Ä–∏—è', '–µ–ª–µ–Ω–∞', '–æ–ª—å–≥–∞', '—Ç–∞—Ç—å—è–Ω–∞', '–Ω–∞—Ç–∞–ª—å—è', '–∏—Ä–∏–Ω–∞', '—é–ª–∏—è',
        '–µ–∫–∞—Ç–µ—Ä–∏–Ω–∞', '–ª—é–¥–º–∏–ª–∞', '—Å–≤–µ—Ç–ª–∞–Ω–∞', '–≥–∞–ª–∏–Ω–∞', '–≤–∞–ª–µ–Ω—Ç–∏–Ω–∞', '–Ω–∞–¥–µ–∂–¥–∞',

        # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –∏–º–µ–Ω–∞ - –ù–ï —Ç—Ä–æ–≥–∞–µ–º
        '–æ–ª–µ–∫—Å–∞–Ω–¥—Ä', '–≤–æ–ª–æ–¥–∏–º–∏—Ä', '—Å–µ—Ä–≥—ñ–π', '–∞–Ω–¥—Ä—ñ–π', '–¥–º–∏—Ç—Ä–æ', '–º–∏–∫–æ–ª–∞', '–æ–ª–µ–∫—Å—ñ–π',
        '–≤—ñ–∫—Ç–æ—Ä', '–ø–µ—Ç—Ä–æ', '—ñ–≤–∞–Ω', '—é—Ä—ñ–π', '–º–∏—Ö–∞–π–ª–æ', '—î–≤–≥–µ–Ω', '–≤–∞—Å–∏–ª—å', '–∞–Ω–∞—Ç–æ–ª—ñ–π',
        '–æ–ª–µ–Ω–∞', '—Ç–µ—Ç—è–Ω–∞', '–Ω–∞—Ç–∞–ª—ñ—è', '—ñ—Ä–∏–Ω–∞', '—é–ª—ñ—è', '–∫–∞—Ç–µ—Ä–∏–Ω–∞', '–ª—é–¥–º–∏–ª–∞', '—Å–≤—ñ—Ç–ª–∞–Ω–∞',

        # –§–∞–º–∏–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã - –ù–ï —Ç—Ä–æ–≥–∞–µ–º
        '–µ–Ω–∫–æ', '–∏—á', '–æ–≤–∏—á', '–µ–≤–∏—á', '—ñ–≤–∏—á', '–æ–≤–Ω–∞', '–µ–≤–Ω–∞', '—ñ–≤–Ω–∞',
        '—Å—å–∫–∏–π', '—Å—å–∫–∞', '—Ü—å–∫–∏–π', '—Ü—å–∫–∞', '—É–∫', '—é–∫', '—á—É–∫'
    ]

    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file)

        for i, row in enumerate(reader):
            if i >= top_n:
                break

            token = row['–¢–æ–∫–µ–Ω'].lower().strip()
            frequency = int(row['–ß–∞—Å—Ç–æ—Ç–∞'])
            percent = float(row['–ü—Ä–æ—Ü–µ–Ω—Ç'].replace('%', ''))

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            if token in STOP_ALL:
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–≤–∏–¥–Ω—ã–µ –∏–º–µ–Ω–∞
            is_likely_name = False
            for name_part in definitely_not_names:
                if name_part in token or token.endswith(('–µ–Ω–∫–æ', '–æ–≤–∏—á', '–µ–≤–∏—á', '—ñ–≤–∏—á', '–æ–≤–Ω–∞', '–µ–≤–Ω–∞', '—ñ–≤–Ω–∞', '—Å—å–∫–∏–π', '—Å—å–∫–∞')):
                    is_likely_name = True
                    break

            if is_likely_name:
                continue

            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤
            categorized = False

            # 1. –ß–∏—Å–ª–∞ –∏ –∫–æ–¥—ã
            for pattern in patterns['numbers_codes']:
                if re.match(pattern, token):
                    mining_categories['numbers_and_codes'].append((token, frequency, percent))
                    categorized = True
                    break

            if not categorized:
                # 2. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                if (len(token) <= 4 and any(char.isdigit() for char in token)) or \
                   any(re.match(pattern, token) for pattern in patterns['fragments']):
                    mining_categories['tech_fragments'].append((token, frequency, percent))
                    categorized = True

                # 3. –ë–∏–∑–Ω–µ—Å —Ç–µ—Ä–º–∏–Ω—ã
                elif any(pattern in token for pattern in patterns['business_patterns']):
                    mining_categories['business_noise'].append((token, frequency, percent))
                    categorized = True

                # 4. –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
                elif any(pattern in token for pattern in patterns['admin_patterns']):
                    mining_categories['administrative'].append((token, frequency, percent))
                    categorized = True

                # 5. –°–µ—Ä–≤–∏—Å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
                elif any(pattern in token for pattern in patterns['service_patterns']):
                    mining_categories['service_operations'].append((token, frequency, percent))
                    categorized = True

                # 6. –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
                elif any(pattern in token for pattern in patterns['descriptive_patterns']) or \
                     token.endswith(('–Ω—ã–π', '–Ω–∞—è', '–Ω–æ–µ', '–Ω—ã–π', '—Å—å–∫–∞', '—Å—å–∫—ñ–π', '–ª—å–Ω—ã–π', '–ª—å–Ω–∞—è')):
                    mining_categories['descriptive_words'].append((token, frequency, percent))
                    categorized = True

                # 7. –ì–ª–∞–≥–æ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã
                elif token.endswith(('–∞—Ç—å', '–∏—Ç—å', '–µ—Ç—å', '—É—Ç—å', '—Ç–∏', '—Ç—Å—è', '—Ç—å—Å—è', '–µ—Ç', '–∏—Ç', '—É—Ç', '—é—Ç')):
                    mining_categories['verb_forms'].append((token, frequency, percent))
                    categorized = True

                # 8. –°–æ—é–∑—ã, –ø—Ä–µ–¥–ª–æ–≥–∏, —á–∞—Å—Ç–∏—Ü—ã
                elif len(token) <= 5 and token in ['–¥–ª—è', '–ø—Ä–∏', '–ø—Ä–æ', '–ø—ñ–¥', '–Ω–∞–¥', '–º—ñ–∂', '—á–µ—Ä–µ–∑', '–ø—ñ—Å–ª—è', '–ø–µ—Ä–µ–¥', '–±—ñ–ª—è', '–∫–æ–ª–æ', '–∞–±–æ', '—á–∏', '—Ç–∞', '—ñ', '–π', '–∞', '–∞–ª–µ', '–æ–¥–Ω–∞–∫', '–ø—Ä–æ—Ç–µ', '—Ç–æ–º—É', '–æ—Å–∫—ñ–ª—å–∫–∏', '—è–∫—â–æ', '–∫–æ–ª–∏', '–¥–µ', '—â–æ', '—è–∫–∏–π', '—è–∫–∞', '—è–∫–µ']:
                    mining_categories['conjunctions_particles'].append((token, frequency, percent))
                    categorized = True

                # 9. –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
                elif any(fin_term in token for fin_term in ['–≥—Ä–∏–≤–Ω', '—Ä—É–±–ª', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '–∫–æ–ø–µ', '—Ü–µ–Ω—Ç', '–≤–∞–ª—é—Ç', '–∫—É—Ä—Å', '–æ–±–º–µ–Ω', '—Ä–∞–∑–º–µ–Ω']):
                    mining_categories['financial_terms'].append((token, frequency, percent))
                    categorized = True

                # 10. –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                elif any(time_ref in token for time_ref in ['—á–∞—Å', '–≤—Ä–µ–º', '–ø–µ—Ä–∏–æ–¥', '—Å—Ä–æ–∫', '–¥–∞—Ç–∞', '—á–∏—Å–ª', '–¥–µ–Ω—å', '–Ω–µ–¥–µ–ª—è', '–º–µ—Å—è—Ü', '–≥–æ–¥', '–≤–µ–∫']):
                    mining_categories['temporal_references'].append((token, frequency, percent))
                    categorized = True

                # 11. –ò–∑–º–µ—Ä–µ–Ω–∏—è –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
                elif any(measure in token for measure in ['–º–µ—Ç—Ä', '–ª–∏—Ç—Ä', '–∫–∏–ª–æ–≥—Ä–∞–º', '—Ç–æ–Ω–Ω', '—à—Ç—É–∫', '–µ–¥–∏–Ω–∏—Ü', '–∫–æ–ª–∏—á–µ—Å—Ç–≤', '—Ä–∞–∑–º–µ—Ä', '–æ–±—ä–µ–º', '–ø–ª–æ—â–∞–¥']):
                    mining_categories['measurement_quantities'].append((token, frequency, percent))
                    categorized = True

                # 12. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ
                elif any(geo in token for geo in ['—Ä–∞–π–æ–Ω', '–æ–∫—Ä—É–≥', '—Å–µ–ª–æ', '–¥–µ—Ä–µ–≤–Ω—è', '–ø–æ—Å–µ–ª–æ–∫', '–º–∏–∫—Ä–æ—Ä–∞–π–æ–Ω', '–∫–≤–∞—Ä—Ç–∞–ª', '–ø—Ä–æ—Å–ø–µ–∫—Ç', '–±—É–ª—å–≤–∞—Ä', '–ø–µ—Ä–µ—É–ª–æ–∫']):
                    mining_categories['geographical_misc'].append((token, frequency, percent))
                    categorized = True

                # 13. –í—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–π –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π –º—É—Å–æ—Ä
                elif frequency > 5000:  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ = –≤–µ—Ä–æ—è—Ç–Ω–æ –º—É—Å–æ—Ä
                    mining_categories['high_freq_garbage'].append((token, frequency, percent))
                    categorized = True

                # 14. –ö–æ—Ä–æ—Ç–∫–∏–π —à—É–º (2-3 —Å–∏–º–≤–æ–ª–∞, –≤—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞)
                elif len(token) <= 3 and frequency > 1000:
                    mining_categories['short_noise'].append((token, frequency, percent))
                    categorized = True

                # 15. –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                elif frequency > 2000 and (token.count('–æ') > len(token)/2 or token.count('–∞') > len(token)/2):
                    mining_categories['suspicious_patterns'].append((token, frequency, percent))
                    categorized = True

    return mining_categories

def print_ultra_mining_results(mining_results):
    """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —É–ª—å—Ç—Ä–∞-–∞–Ω–∞–ª–∏–∑–∞"""

    print(f"\nüî• –†–ï–ó–£–õ–¨–¢–ê–¢–´ –£–õ–¨–¢–†–ê-–ì–õ–£–ë–û–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê")
    print("=" * 80)

    total_found = sum(len(tokens) for tokens in mining_results.values())
    print(f"üéØ –í–°–ï–ì–û –ù–ê–ô–î–ï–ù–û –ö–ê–ù–î–ò–î–ê–¢–û–í: {total_found}")

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    priority_categories = [
        'high_freq_garbage', 'numbers_and_codes', 'short_noise',
        'tech_fragments', 'conjunctions_particles'
    ]

    priority_tokens = []

    for category, tokens in mining_results.items():
        if not tokens:
            continue

        print(f"\nüìÇ {category.upper().replace('_', ' ')} ({len(tokens)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤):")

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        sorted_tokens = sorted(tokens, key=lambda x: x[1], reverse=True)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-20 –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π, —Ç–æ–ø-10 –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
        show_count = 20 if category in priority_categories else 10

        for i, (token, freq, percent) in enumerate(sorted_tokens[:show_count], 1):
            print(f"   {i:2}. {token:<15} (freq: {freq:>6}, {percent:>6.3f}%)")

            # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            if category in priority_categories and freq > 500:  # –¢–æ–ª—å–∫–æ –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–µ
                priority_tokens.append(token)

        if len(sorted_tokens) > show_count:
            print(f"       ... –∏ –µ—â–µ {len(sorted_tokens) - show_count}")

    return priority_tokens

def generate_ultra_additions(priority_tokens):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞"""

    print(f"\n" + "=" * 80)
    print("üí• –ö–û–î –î–õ–Ø –£–õ–¨–¢–†–ê-–î–û–ü–û–õ–ù–ï–ù–ò–Ø –°–¢–û–ü-–°–õ–û–í")
    print("=" * 80)

    print(f"\n# –£–ª—å—Ç—Ä–∞-–≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ 1–ú —Ç–æ–∫–µ–Ω–æ–≤ - —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –∑–∞—á–∏—Å—Ç–∫–∞:")
    print(f"# –ù–∞–π–¥–µ–Ω–æ {len(priority_tokens)} –≤—ã—Å–æ–∫–æ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –º—É—Å–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤")

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≥—Ä—É–ø–ø—ã –ø–æ 10 –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    for i in range(0, len(priority_tokens), 10):
        batch = priority_tokens[i:i+10]
        formatted = [f'"{token}"' for token in batch]
        print("    " + ", ".join(formatted) + ",")

    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –£–õ–¨–¢–†–ê-–ê–ù–ê–õ–ò–ó–ê:")
    print(f"   ‚Ä¢ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: —Ç–æ–ø-10,000 —Ç–æ–∫–µ–Ω–æ–≤")
    print(f"   ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(priority_tokens)}")
    print(f"   ‚Ä¢ –¢–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä: {len(STOP_ALL)} —Å—Ç–æ–ø-—Å–ª–æ–≤")
    print(f"   ‚Ä¢ –ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: ~{len(STOP_ALL) + len(priority_tokens)} —Å—Ç–æ–ø-—Å–ª–æ–≤")
    print(f"   ‚Ä¢ –û–∂–∏–¥–∞–µ–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: +2-5%")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —É–ª—å—Ç—Ä–∞-–∞–Ω–∞–ª–∏–∑–∞"""

    mining_results = ultra_deep_mining("all_tokens_by_frequency.csv", top_n=10000)
    priority_tokens = print_ultra_mining_results(mining_results)

    if priority_tokens:
        generate_ultra_additions(priority_tokens[:50])  # –¢–æ–ø-50 –¥–ª—è –Ω–∞—á–∞–ª–∞

    print(f"\n‚úÖ –£–ª—å—Ç—Ä–∞-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –ì–æ—Ç–æ–≤–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∏—è!")

if __name__ == "__main__":
    main()