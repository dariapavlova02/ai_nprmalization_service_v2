#!/usr/bin/env python3
"""
–û—Ç–ª–∞–¥–∫–∞ —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ª–∞—Ç–∏–Ω—Å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏ –∏ J.
"""

import os
import sys
from pathlib import Path

# Set production environment variables
os.environ.update({
    'PRESERVE_FEMININE_SURNAMES': 'true',
    'ENABLE_ENHANCED_GENDER_RULES': 'true',
    'PRESERVE_FEMININE_SUFFIX_UK': 'true',
    'ENABLE_FSM_TUNED_ROLES': 'true',
    'MORPHOLOGY_CUSTOM_RULES_FIRST': 'true',
    'ENABLE_ADVANCED_FEATURES': 'true',
    'NORMALIZATION_ENABLE_ADVANCED_FEATURES': 'true',
    'NORMALIZATION_ENABLE_MORPHOLOGY': 'true',
    'ENFORCE_NOMINATIVE': 'false',
    'DEBUG_TRACING': 'true'
})

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def analyze_text_characters(text):
    """–ê–Ω–∞–ª–∏–∑ —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ."""
    print(f"\nüîç –ê–ù–ê–õ–ò–ó –°–ò–ú–í–û–õ–û–í –í –¢–ï–ö–°–¢–ï: '{text}'")
    for i, char in enumerate(text):
        print(f"  [{i}] '{char}' -> U+{ord(char):04X} ({char.encode('unicode_escape').decode()})")

def test_mixed_content():
    """–¢–µ—Å—Ç —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    print("üîç –¢–ï–°–¢ –°–ú–ï–®–ê–ù–ù–û–ì–û –ö–û–ù–¢–ï–ù–¢–ê")
    print("="*60)

    # –ü—Ä–æ–±–ª–µ–º–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    text = "–û–ø–ªa—Ça –üepo—àe–Ω–∫a O—Ç J –üe—Çpa"
    analyze_text_characters(text)

    try:
        from ai_service.layers.normalization.normalization_service import NormalizationService

        service = NormalizationService()

        print(f"\nüìù –¢–µ—Å—Ç: '{text}'")

        result = service.normalize_sync(
            text=text,
            language=None,  # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
            remove_stop_words=True,
            preserve_names=True,
            enable_advanced_features=True
        )

        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: '{result.normalized}'")
        print(f"‚úÖ –Ø–∑—ã–∫: '{result.language}'")
        print(f"‚úÖ –¢–æ–∫–µ–Ω—ã: {result.tokens}")

        print(f"\nüîç TRACE –ê–ù–ê–õ–ò–ó:")
        for i, trace in enumerate(result.trace):
            if hasattr(trace, 'token'):
                print(f"  {i:2d}: '{trace.token}' ‚Üí {trace.role} ‚Üí '{trace.output}'")
                if trace.notes:
                    notes = trace.notes[:100] + "..." if len(trace.notes) > 100 else trace.notes
                    print(f"      notes: {notes}")
            else:
                print(f"  {i:2d}: {trace}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

def test_clean_version():
    """–¢–µ—Å—Ç —Å –æ—á–∏—â–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π."""
    print("\nüîç –¢–ï–°–¢ –û–ß–ò–©–ï–ù–ù–û–ô –í–ï–†–°–ò–ò")
    print("="*40)

    # –û—á–∏—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
    clean_text = "–û–ø–ª–∞—Ç–∞ –ü–æ—Ä–æ—à–µ–Ω–∫–∞ –û—Ç J –ü–µ—Ç—Ä–∞"
    analyze_text_characters(clean_text)

    try:
        from ai_service.layers.normalization.normalization_service import NormalizationService

        service = NormalizationService()

        print(f"\nüìù –¢–µ—Å—Ç: '{clean_text}'")

        result = service.normalize_sync(
            text=clean_text,
            language="uk",  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π
            remove_stop_words=True,
            preserve_names=True,
            enable_advanced_features=True
        )

        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: '{result.normalized}'")
        print(f"‚úÖ –Ø–∑—ã–∫: '{result.language}'")
        print(f"‚úÖ –¢–æ–∫–µ–Ω—ã: {result.tokens}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    test_mixed_content()
    test_clean_version()