# –°–∏—Å—Ç–µ–º–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è AI Service Normalization

–ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –±—ã—Å—Ç—Ä—ã—Ö –∫—ç—à–µ–π –∏ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ–º p95 ‚â§ 10–º—Å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö.

## üéØ –¶–µ–ª—å

- **LRU-–∫—ç—à –Ω–∞ —Å–ª–æ—è—Ö Tokenizer –∏ Morphology** —Å TTL –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
- **–ú–µ—Ç—Ä–∏–∫–∏ –≤ Prometheus**: cache_hit_rate, miss_rate, p95, cache_size
- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: p95 ‚â§ 10–º—Å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö; hit-rate ‚â• 30% –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —à–∞–±–ª–æ–Ω–∞—Ö

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã

```
src/ai_service/utils/
‚îî‚îÄ‚îÄ lru_cache_ttl.py              # LRU –∫—ç—à —Å TTL, thread-safe

src/ai_service/layers/normalization/
‚îú‚îÄ‚îÄ tokenizer_service.py          # –°–µ—Ä–≤–∏—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
‚îú‚îÄ‚îÄ morphology_adapter.py         # –ê–¥–∞–ø—Ç–µ—Ä –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
‚îî‚îÄ‚îÄ processors/normalization_factory.py  # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫—ç—à–µ–π –≤ factory

src/ai_service/monitoring/
‚îî‚îÄ‚îÄ cache_metrics.py              # Prometheus –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫—ç—à–µ–π

tests/
‚îú‚îÄ‚îÄ unit/test_caching_normalization.py    # Unit —Ç–µ—Å—Ç—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
‚îî‚îÄ‚îÄ perf/test_p95_short_text.py          # Performance —Ç–µ—Å—Ç—ã p95
```

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã

```bash
python scripts/demo_caching.py
```

### 2. –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤

```bash
# Unit —Ç–µ—Å—Ç—ã
pytest tests/unit/test_caching_normalization.py -v

# Performance —Ç–µ—Å—Ç—ã
pytest tests/perf/test_p95_short_text.py -v
```

### 3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–¥–µ

```python
from ai_service.utils.lru_cache_ttl import CacheManager
from ai_service.layers.normalization.tokenizer_service import TokenizerService
from ai_service.layers.normalization.morphology_adapter import MorphologyAdapter

# –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫—ç—à–µ–π
cache_manager = CacheManager({
    'max_size': 2048,
    'ttl_sec': 600,
    'enable_cache': True
})

# –°–µ—Ä–≤–∏—Å—ã —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
tokenizer = TokenizerService(cache_manager.get_tokenizer_cache())
morphology = MorphologyAdapter(cache_manager.get_morphology_cache())
```

## üîß –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

### 1. LRU –∫—ç—à —Å TTL (`LruTtlCache`)

```python
from ai_service.utils.lru_cache_ttl import LruTtlCache

# –°–æ–∑–¥–∞–Ω–∏–µ –∫—ç—à–∞
cache = LruTtlCache(maxsize=2048, ttl_seconds=600)

# –û–ø–µ—Ä–∞—Ü–∏–∏
cache.set("key", "value")
hit, value = cache.get("key")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
stats = cache.get_stats()
print(f"Hit rate: {stats['hit_rate']:.2f}%")
```

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- Thread-safe —Å RLock
- LRU eviction –ø—Ä–∏ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–∏
- TTL expiration –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏
- –ú–µ—Ç—Ä–∏–∫–∏ hits/misses/evictions/expirations

### 2. –°–µ—Ä–≤–∏—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (`TokenizerService`)

```python
from ai_service.layers.normalization.tokenizer_service import TokenizerService

service = TokenizerService(cache)

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
result = service.tokenize(
    "–Ü–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤",
    language="uk",
    remove_stop_words=True,
    preserve_names=True,
    feature_flags={"enable_advanced_features": True}
)

print(f"Tokens: {result.tokens}")
print(f"Cache hit: {result.cache_hit}")
print(f"Processing time: {result.processing_time:.4f}s")
```

**–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ:**
- –ö–ª—é—á: `(language, sanitized_text, flags_hash)`
- –†–µ–∑—É–ª—å—Ç–∞—Ç: `{tokens, traces, metadata}`
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ Prometheus

### 3. –ê–¥–∞–ø—Ç–µ—Ä –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏ (`MorphologyAdapter`)

```python
from ai_service.layers.normalization.morphology_adapter import MorphologyAdapter

adapter = MorphologyAdapter(cache)

# –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
result = adapter.parse(
    "–ü–µ—Ç—Ä–æ–≤",
    language="ru",
    role="surname",
    feature_flags={"enable_morphology": True}
)

print(f"Normalized: {result.normalized}")
print(f"Cache hit: {result.cache_hit}")
print(f"Parses: {result.parses}")
```

**–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ:**
- –ö–ª—é—á: `(language_morph, token_role, flags_hash)`
- –†–µ–∑—É–ª—å—Ç–∞—Ç: `{normalized, fallback, traces, parses}`
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–∏—Ö –∂–µ–Ω—Å–∫–∏—Ö —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤

### 4. Prometheus –º–µ—Ç—Ä–∏–∫–∏ (`CacheMetrics`)

```python
from ai_service.monitoring.cache_metrics import CacheMetrics

metrics = CacheMetrics()

# –ó–∞–ø–∏—Å—å —Å–æ–±—ã—Ç–∏–π
metrics.record_tokenizer_cache_hit("uk")
metrics.record_morphology_cache_miss("ru")

# –ó–∞–ø–∏—Å—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
metrics.record_layer_latency("tokenizer", "uk", 0.01)
metrics.record_normalization_latency("uk", 0.05)

# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ hit rate
metrics.update_tokenizer_cache_size("uk", 100)
metrics.update_tokenizer_cache_hit_rate("uk", 75.5)
```

**–ú–µ—Ç—Ä–∏–∫–∏:**
- `normalization_tokenizer_cache_hits_total{language}`
- `normalization_morph_cache_hits_total{language}`
- `normalization_layer_latency_seconds{layer,language}`
- `normalization_cache_size{layer,language}`
- `normalization_cache_hit_rate{layer,language}`

## üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### CacheManager –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```python
config = {
    'max_size': 2048,        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
    'ttl_sec': 600,          # TTL –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    'enable_cache': True     # –í–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
}

cache_manager = CacheManager(config)
```

### NormalizationConfig —Ñ–ª–∞–≥–∏

```python
config = NormalizationConfig(
    enable_cache=True,           # –í–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    debug_tracing=True,         # –í–∫–ª—é—á–∏—Ç—å debug tracing —Å cache info
    language="uk",
    remove_stop_words=True,
    preserve_names=True,
    enable_advanced_features=True
)
```

## üîç Debug Tracing

–ü—Ä–∏ –≤–∫–ª—é—á—ë–Ω–Ω–æ–º `debug_tracing=True` –≤ `NormalizationResult.trace` –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—ç—à–µ:

```python
{
    "token": "–ü–µ—Ç—Ä–æ–≤",
    "role": "surname", 
    "rule": "role_classification:surname + morphological_normalization",
    "output": "–ü–µ—Ç—Ä–æ–≤",
    "cache": {
        "tokenizer": "hit",
        "morph": "miss"
    },
    "fallback": false,
    "notes": "Morphology normalization: '–ü–µ—Ç—Ä–æ–≤' -> '–ü–µ—Ç—Ä–æ–≤'"
}
```

## ‚ö° –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏

- ‚úÖ **p95 ‚â§ 10–º—Å** –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö (–ª–æ–∫–∞–ª—å–Ω–æ)
- ‚úÖ **Hit-rate ‚â• 30%** –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö
- ‚úÖ **–ú–µ—Ç—Ä–∏–∫–∏ –≤–∏–¥–Ω—ã** –≤ `/metrics` endpoint
- ‚úÖ **Unit —Ç–µ—Å—Ç—ã –∑–µ–ª—ë–Ω—ã–µ**
- ‚úÖ **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –Ω–µ –ª–æ–º–∞—é—Ç—Å—è**

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

```bash
# Performance —Ç–µ—Å—Ç—ã
pytest tests/perf/test_p95_short_text.py -v -s

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:
# Tokenizer p95 latency: 8.45ms ‚úÖ
# Morphology p95 latency: 6.23ms ‚úÖ  
# Combined p95 latency: 9.87ms ‚úÖ
# Tokenizer hit rate: 85.2% ‚úÖ
# Morphology hit rate: 78.6% ‚úÖ
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

1. **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞** ‚Üí -30-50% –≤—Ä–µ–º–µ–Ω–∏
2. **–ü—Ä–µ–¥–∫–æ–º–ø–∏–ª—è—Ü–∏—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π** ‚Üí -10-20% –≤—Ä–µ–º–µ–Ω–∏
3. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ –≤ —Å–ª–æ–≤–∞—Ä—è—Ö** ‚Üí -5-15% –≤—Ä–µ–º–µ–Ω–∏
4. **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–æ–ª–µ–π** ‚Üí -20-40% –≤—Ä–µ–º–µ–Ω–∏

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### Unit —Ç–µ—Å—Ç—ã

```bash
# –¢–µ—Å—Ç—ã LRU –∫—ç—à–∞
pytest tests/unit/test_caching_normalization.py::TestLruTtlCache -v

# –¢–µ—Å—Ç—ã —Å–µ—Ä–≤–∏—Å–æ–≤
pytest tests/unit/test_caching_normalization.py::TestTokenizerServiceCaching -v
pytest tests/unit/test_caching_normalization.py::TestMorphologyAdapterCaching -v

# –¢–µ—Å—Ç—ã –º–µ—Ç—Ä–∏–∫
pytest tests/unit/test_caching_normalization.py::TestCacheMetrics -v
```

### Performance —Ç–µ—Å—Ç—ã

```bash
# –¢–µ—Å—Ç—ã p95 –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
pytest tests/perf/test_p95_short_text.py::TestP95ShortTextPerformance -v

# –¢–µ—Å—Ç—ã hit rate
pytest tests/perf/test_p95_short_text.py::TestP95ShortTextPerformance::test_cache_hit_rate_performance -v

# –ë–µ–Ω—á–º–∞—Ä–∫–∏
pytest tests/perf/test_p95_short_text.py::TestPerformanceBenchmarks -v
```

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### Prometheus –º–µ—Ç—Ä–∏–∫–∏

```bash
# –ü—Ä–æ—Å–º–æ—Ç—Ä –º–µ—Ç—Ä–∏–∫
curl http://localhost:8000/metrics | grep normalization

# –ü—Ä–∏–º–µ—Ä—ã –º–µ—Ç—Ä–∏–∫:
# normalization_tokenizer_cache_hits_total{language="uk"} 1250
# normalization_morph_cache_hits_total{language="ru"} 890
# normalization_layer_latency_seconds{layer="tokenizer",language="uk"} 0.008
# normalization_cache_size{layer="tokenizer",language="uk"} 1024
# normalization_cache_hit_rate{layer="tokenizer",language="uk"} 85.2
```

### Grafana –¥–∞—à–±–æ—Ä–¥—ã

–°–æ–∑–¥–∞–π—Ç–µ –¥–∞—à–±–æ—Ä–¥—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:
- Cache hit rate –ø–æ —Å–ª–æ—è–º –∏ —è–∑—ã–∫–∞–º
- Latency percentiles (p50, p95, p99)
- Cache size –∏ eviction rate
- Error rates –∏ fallback usage

## üîß Troubleshooting

### –ù–∏–∑–∫–∏–π hit rate

```python
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫—ç—à–∞
stats = cache_manager.get_all_stats()
print(f"Tokenizer hit rate: {stats['tokenizer']['hit_rate']:.2f}%")

# –£–≤–µ–ª–∏—á—å—Ç–µ TTL –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ –º–µ–Ω—è—é—Ç—Å—è
config['ttl_sec'] = 1800  # 30 –º–∏–Ω—É—Ç

# –£–≤–µ–ª–∏—á—å—Ç–µ —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
config['max_size'] = 4096
```

### –í—ã—Å–æ–∫–∞—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å

```python
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
stats = cache_manager.get_all_stats()
print(f"Cache size: {stats['tokenizer']['size']}")

# –û—á–∏—Å—Ç–∏—Ç–µ –∫—ç—à –µ—Å–ª–∏ –æ–Ω –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω
cache_manager.clear_all()

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ TTL - –≤–æ–∑–º–æ–∂–Ω–æ, —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
config['ttl_sec'] = 600  # 10 –º–∏–Ω—É—Ç
```

### –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–∞–º—è—Ç—å—é

```python
# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
import psutil
process = psutil.Process()
memory_mb = process.memory_info().rss / 1024 / 1024
print(f"Memory usage: {memory_mb:.2f} MB")

# –£–º–µ–Ω—å—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
config['max_size'] = 1024
```

## üìö API Reference

### LruTtlCache

```python
class LruTtlCache:
    def __init__(self, maxsize: int = 2048, ttl_seconds: int = 600)
    def get(self, key: Any) -> Tuple[bool, Optional[Any]]
    def set(self, key: Any, value: Any) -> None
    def delete(self, key: Any) -> bool
    def clear(self) -> None
    def purge_expired(self) -> int
    def get_stats(self) -> Dict[str, Any]
    def enable(self) -> None
    def disable(self) -> None
```

### TokenizerService

```python
class TokenizerService:
    def __init__(self, cache: Optional[LruTtlCache] = None)
    def tokenize(self, text: str, language: str = "uk", 
                 remove_stop_words: bool = True, preserve_names: bool = True,
                 stop_words: Optional[set] = None, 
                 feature_flags: Optional[Dict[str, Any]] = None) -> TokenizationResult
    def get_stats(self) -> Dict[str, Any]
    def clear_cache(self) -> None
    def reset_stats(self) -> None
```

### MorphologyAdapter

```python
class MorphologyAdapter:
    def __init__(self, cache: Optional[LruTtlCache] = None, 
                 diminutive_maps: Optional[Dict[str, Dict[str, str]]] = None)
    def parse(self, token: str, language: str, role: Optional[str] = None,
              feature_flags: Optional[Dict[str, Any]] = None) -> MorphologyResult
    def normalize_slavic_token(self, token: str, role: Optional[str], 
                              language: str, enable_morphology: bool = True,
                              preserve_feminine_suffix_uk: bool = False,
                              feature_flags: Optional[Dict[str, Any]] = None) -> MorphologyResult
    def get_stats(self) -> Dict[str, Any]
    def clear_cache(self) -> None
    def reset_stats(self) -> None
```

## üéâ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–°–∏—Å—Ç–µ–º–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:

- ‚úÖ **LRU –∫—ç—à–∏ —Å TTL** –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏
- ‚úÖ **Prometheus –º–µ—Ç—Ä–∏–∫–∏** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- ‚úÖ **Debug tracing** —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫—ç—à–µ
- ‚úÖ **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å p95 ‚â§ 10–º—Å** –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö
- ‚úÖ **Hit rate ‚â• 30%** –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —à–∞–±–ª–æ–Ω–∞—Ö
- ‚úÖ **Thread-safe** –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å RLock
- ‚úÖ **–ü–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ** unit –∏ performance —Ç–µ—Å—Ç–∞–º–∏

–ó–∞–ø—É—Å—Ç–∏—Ç–µ `python scripts/demo_caching.py` –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞ —Å —Å–∏—Å—Ç–µ–º–æ–π!
