#!/usr/bin/env python3
"""
Deep analysis of remaining tokens from 1M payments to find ALL possible stopwords
"""

import csv
import re
import sys
sys.path.append('/Users/dariapavlova/Desktop/ai-service/src')

from ai_service.data.dicts.stopwords import STOP_ALL

def deep_analyze_stopwords(filename, top_n=5000):
    """Comprehensive analysis of ALL remaining tokens"""

    print(f"üîç DEEP ANALYSIS of top {top_n} tokens...")
    print(f"üìö Current stopwords: {len(STOP_ALL)}")

    categories = {
        'prepositions_conjunctions': [],  # –ü—Ä–µ–¥–ª–æ–≥–∏, —Å–æ—é–∑—ã, —á–∞—Å—Ç–∏—Ü—ã
        'verbs_adjectives': [],          # –ì–ª–∞–≥–æ–ª—ã, –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ
        'numbers_codes': [],             # –ß–∏—Å–ª–∞, –∫–æ–¥—ã, –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        'fragments_suffixes': [],        # –§—Ä–∞–≥–º–µ–Ω—Ç—ã, —Å—É—Ñ—Ñ–∏–∫—Å—ã, –æ–±—Ä—ã–≤–∫–∏
        'company_brands': [],            # –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, –±—Ä–µ–Ω–¥—ã
        'tech_abbreviations': [],        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        'service_terms': [],             # –°–µ—Ä–≤–∏—Å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        'suspicious_high_freq': [],      # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ —á–∞—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã
        'short_garbage': [],             # –ö–æ—Ä–æ—Ç–∫–∏–π –º—É—Å–æ—Ä
        'descriptive_terms': []          # –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    }

    # Expanded patterns for better categorization
    preposition_patterns = [
        '–¥–ª—è', '–≤—ñ–¥', '–ø—Ä–∏', '–ø—Ä–æ', '–ø—ñ–¥', '–Ω–∞–¥', '–º—ñ–∂', '—á–µ—Ä–µ–∑', '–ø—ñ—Å–ª—è', '–ø–µ—Ä–µ–¥',
        '–±—ñ–ª—è', '–∫–æ–ª–æ', '–Ω–∞–≤–∫–æ–ª–æ', '–≤—Å–µ—Ä–µ–¥–∏–Ω—ñ', '–∑–æ–≤–Ω—ñ', '–ø–æ—Ä—É—á', '–¥–∞–ª—ñ', '–±–ª–∏–∂—á–µ'
    ]

    verb_patterns = [
        '—î', '–º–∞—î', '–±—É–¥–µ', '–±—É–≤', '–±—É–ª–∞', '–±—É–ª–∏', '—Ä–æ–±–∏—Ç—å', '—Ä–æ–±–∏–≤', '–∑—Ä–æ–±–∏–≤',
        '–∫–∞–∂–µ', '–∫–∞–∑–∞–≤', '—Å–∫–∞–∑–∞–≤', '–π–¥–µ', '–π—à–æ–≤', '–ø—ñ—à–æ–≤', '–¥–∞—î', '–¥–∞–≤–∞–≤', '–¥–∞–≤'
    ]

    adjective_patterns = [
        '–Ω–æ–≤–∏–π', '—Å—Ç–∞—Ä–∏–π', '–≤–µ–ª–∏–∫–∏–π', '–º–∞–ª–∏–π', '—Ö–æ—Ä–æ—à–∏–π', '–ø–æ–≥–∞–Ω–∏–π', '—à–≤–∏–¥–∫–∏–π',
        '–ø–æ–≤—ñ–ª—å–Ω–∏–π', '–≤–∏—Å–æ–∫–∏–π', '–Ω–∏–∑—å–∫–∏–π', '–¥–æ–≤–≥–∏–π', '–∫–æ—Ä–æ—Ç–∫–∏–π', '—à–∏—Ä–æ–∫–∏–π', '–≤—É–∑—å–∫–∏–π'
    ]

    # Known name patterns (to avoid removing real names)
    definite_name_patterns = [
        '–æ–ª–µ–∫—Å–∞–Ω–¥—Ä', '–≤–æ–ª–æ–¥–∏–º–∏—Ä', '—Å–µ—Ä–≥—ñ–π', '–∞–Ω–¥—Ä—ñ–π', '–¥–º–∏—Ç—Ä–æ', '–º–∏–∫–æ–ª–∞', '–æ–ª–µ–∫—Å—ñ–π',
        '–≤—ñ–∫—Ç–æ—Ä', '–ø–µ—Ç—Ä–æ', '—ñ–≤–∞–Ω', '—é—Ä—ñ–π', '–º–∏—Ö–∞–π–ª–æ', '—î–≤–≥–µ–Ω', '–≤–∞—Å–∏–ª—å', '–∞–Ω–∞—Ç–æ–ª—ñ–π',
        '–∞–Ω–Ω–∞', '–º–∞—Ä—ñ—è', '–æ–ª–µ–Ω–∞', '–æ–ª—å–≥–∞', '—Ç–µ—Ç—è–Ω–∞', '–Ω–∞—Ç–∞–ª—ñ—è', '—ñ—Ä–∏–Ω–∞', '—é–ª—ñ—è',
        '–∫–∞—Ç–µ—Ä–∏–Ω–∞', '–ª—é–¥–º–∏–ª–∞', '—Å–≤—ñ—Ç–ª–∞–Ω–∞', '–≥–∞–ª–∏–Ω–∞', '–≤–∞–ª–µ–Ω—Ç–∏–Ω–∞'
    ]

    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file)

        for i, row in enumerate(reader):
            if i >= top_n:
                break

            token = row['–¢–æ–∫–µ–Ω'].lower().strip()
            frequency = int(row['–ß–∞—Å—Ç–æ—Ç–∞'])
            percent = float(row['–ü—Ä–æ—Ü–µ–Ω—Ç'].replace('%', ''))

            # Skip if already in stopwords
            if token in STOP_ALL:
                continue

            # Skip very short tokens (but collect them separately)
            if len(token) <= 2:
                if frequency > 1000:  # High frequency short tokens are likely garbage
                    categories['short_garbage'].append((token, frequency, percent))
                continue

            # Skip if it's a definite name
            if any(name_part in token for name_part in definite_name_patterns):
                continue

            # Skip patronymic/surname patterns
            if (token.endswith(('–æ–≤–∏—á', '–µ–≤–∏—á', '—ñ–≤–∏—á', '–æ–≤–Ω–∞', '–µ–≤–Ω–∞', '—ñ–≤–Ω–∞')) or
                token.endswith(('–µ–Ω–∫–æ', '—É–∫', '—é–∫', '—á—É–∫', '—Å—å–∫–∏–π', '—Å—å–∫–∞'))):
                continue

            # Categorize tokens
            categorized = False

            # 1. Prepositions and conjunctions
            if token in preposition_patterns or any(prep in token for prep in preposition_patterns[:5]):
                categories['prepositions_conjunctions'].append((token, frequency, percent))
                categorized = True

            # 2. Verbs and adjectives
            elif (token in verb_patterns or token in adjective_patterns or
                  token.endswith(('–∏—Ç—å', '–∞—Ç—å', '–µ—Ç—å', '—É—Ç–∏', '–∏–π', '—ã–π', '–æ–π', '–∞—è', '–µ–µ'))):
                categories['verbs_adjectives'].append((token, frequency, percent))
                categorized = True

            # 3. Numbers, codes, technical identifiers
            elif (re.match(r'^[0-9a-f]{6,}$', token) or  # Hex codes
                  re.match(r'^[a-z]{2,4}[0-9]+$', token) or  # Code patterns
                  token.count('0') > len(token)/3):  # Lots of zeros
                categories['numbers_codes'].append((token, frequency, percent))
                categorized = True

            # 4. Company names and brands
            elif (token.endswith(('–ª—Ç–¥', '–∏–Ω–∫', '–∫–æ—Ä–ø', '–≥—Ä—É–ø')) or
                  token.startswith(('—É–∫—Ä', '–∫–∏–π', '–æ–¥–µ—Å', '—Ö–∞—Ä–∫—ñ–≤', '–ª—å–≤—ñ–≤')) or
                  any(brand in token for brand in ['—Ñ–µ–π—Å–±—É–∫', '–≥—É–≥–ª', '–∞–º–∞–∑–æ–Ω', '–º–∞–π–∫—Ä–æ—Å–æ—Ñ—Ç'])):
                categories['company_brands'].append((token, frequency, percent))
                categorized = True

            # 5. Technical abbreviations (all caps, 3-6 letters)
            elif (token.isupper() and 3 <= len(token) <= 6 and
                  not any(char.isdigit() for char in token)):
                categories['tech_abbreviations'].append((token, frequency, percent))
                categorized = True

            # 6. Service terms
            elif any(service in token for service in [
                '—Å–µ—Ä–≤—ñ—Å', '—Å–ª—É–∂–±', '–≤—ñ–¥–¥—ñ–ª', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '—É–ø—Ä–∞–≤–ª—ñ–Ω–Ω', '–∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ü',
                '—Ü–µ–Ω—Ç—Ä', '–æ—Ñ—ñ—Å', '—Ñ—ñ–ª—ñ', '–≤—ñ–¥–¥—ñ–ª–µ–Ω–Ω', '–ø—Ä–µ–¥—Å—Ç–∞–≤–Ω–∏—Ü—Ç–≤'
            ]):
                categories['service_terms'].append((token, frequency, percent))
                categorized = True

            # 7. Fragments and suffixes (likely parts of compound words)
            elif (len(token) <= 5 and
                  (token.startswith(('–Ω–µ', '–ø—Ä–µ', '–ø—Ä–æ', '–ø—ñ–¥', '–Ω–∞–¥', '—Ä–æ–∑', '–±–µ–∑')) or
                   token.endswith(('–Ω–Ω—è', '—Ç—Å—è', '—Ç—å—Å—è', '—ñ—Å—Ç—å', '–Ω—ñ—Å—Ç—å', '–ª—å–Ω')))):
                categories['fragments_suffixes'].append((token, frequency, percent))
                categorized = True

            # 8. Descriptive terms (adjectives, adverbs, etc.)
            elif (token.endswith(('–Ω–∏–π', '–Ω–∏–π', '—Å—å–∫–∞', '—Ü—å–∫–∞', '–ª—å–Ω', '–æ–≤—ñ', '–µ–≤—ñ')) and
                  len(token) > 5):
                categories['descriptive_terms'].append((token, frequency, percent))
                categorized = True

            # 9. High frequency suspicious tokens
            elif frequency > 2000:  # Very high frequency, probably not a name
                categories['suspicious_high_freq'].append((token, frequency, percent))
                categorized = True

    return categories

def print_deep_analysis(categories):
    """Print comprehensive analysis results"""

    print(f"\n" + "="*80)
    print(f"üî¨ COMPREHENSIVE STOPWORDS ANALYSIS")
    print(f"="*80)

    total_found = sum(len(tokens) for tokens in categories.values())
    print(f"\nüéØ TOTAL CANDIDATES: {total_found}")

    # Detailed breakdown by category
    for category, tokens in categories.items():
        if not tokens:
            continue

        print(f"\nüìÇ {category.upper().replace('_', ' ')} ({len(tokens)} candidates):")

        # Sort by frequency
        sorted_tokens = sorted(tokens, key=lambda x: x[1], reverse=True)

        # Show top 20 for high-impact categories
        show_count = 20 if category in ['suspicious_high_freq', 'fragments_suffixes', 'short_garbage'] else 10

        for i, (token, freq, percent) in enumerate(sorted_tokens[:show_count], 1):
            print(f"   {i:2}. {token:<15} (freq: {freq:>6}, {percent:>6.3f}%)")

        if len(sorted_tokens) > show_count:
            print(f"       ... and {len(sorted_tokens) - show_count} more")

    # Generate priority recommendations
    print(f"\nüöÄ PRIORITY RECOMMENDATIONS:")

    # High impact categories
    high_impact = []
    for category in ['suspicious_high_freq', 'prepositions_conjunctions', 'short_garbage']:
        if category in categories:
            high_impact.extend([token for token, freq, _ in categories[category] if freq > 1000])

    if high_impact:
        print(f"\nüî• IMMEDIATE ACTION (high frequency garbage):")
        for i in range(0, min(30, len(high_impact)), 10):
            batch = high_impact[i:i+10]
            print(f"   {', '.join(batch)}")

    # Medium impact categories
    medium_impact = []
    for category in ['fragments_suffixes', 'verbs_adjectives', 'descriptive_terms']:
        if category in categories:
            medium_impact.extend([token for token, freq, _ in categories[category][:5] if freq > 500])

    if medium_impact:
        print(f"\n‚ö†Ô∏è  REVIEW AND ADD (medium frequency):")
        for i in range(0, min(20, len(medium_impact)), 10):
            batch = medium_impact[i:i+10]
            print(f"   {', '.join(batch)}")

    return high_impact + medium_impact

def generate_stopwords_code(recommendations):
    """Generate code for adding to stopwords.py"""

    print(f"\n" + "="*80)
    print(f"üìù CODE FOR STOPWORDS.PY:")
    print(f"="*80)

    print(f"\n# Final cleanup - high-frequency non-name tokens from deep analysis:")

    # Split into logical groups for better organization
    for i in range(0, len(recommendations), 8):
        batch = recommendations[i:i+8]
        formatted = [f'"{term}"' for term in batch]
        print("    " + ", ".join(formatted) + ",")

def main():
    """Run deep stopwords analysis"""

    categories = deep_analyze_stopwords("all_tokens_by_frequency.csv", top_n=5000)
    recommendations = print_deep_analysis(categories)

    if recommendations:
        generate_stopwords_code(recommendations[:40])  # Top 40 recommendations

    print(f"\n‚úÖ Analysis complete. Found {len(recommendations)} high-priority additions.")

if __name__ == "__main__":
    main()