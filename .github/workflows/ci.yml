name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Канареечные тесты каждый день в 6:00 UTC
    - cron: "0 6 * * *"

jobs:
  test:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: [3.12]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true
    
    - name: Load cached venv
      id: cached-poetry-dependencies
      uses: actions/cache@v3
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}
    
    - name: Install dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install --no-dev
    
    - name: Install dev dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install
    
    - name: Install optional spaCy models
      run: |
        python -m spacy download ru_core_news_sm || true
        python -m spacy download uk_core_news_sm || true
        python -m spacy download en_core_web_sm || true
    
    - name: Run tests
      run: poetry run pytest tests/ -v --tb=short
    
    - name: Run tests with coverage
      run: poetry run pytest tests/ --cov=src --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

  canary_run:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    strategy:
      matrix:
        python-version: [3.12]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true
    
    - name: Load cached venv
      id: cached-poetry-dependencies
      uses: actions/cache@v3
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}
    
    - name: Install dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install --no-dev
    
    - name: Install dev dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install
    
    - name: Install optional spaCy models
      run: |
        python -m spacy download ru_core_news_sm || true
        python -m spacy download uk_core_news_sm || true
        python -m spacy download en_core_web_sm || true
    
    - name: Create artifacts directory
      run: mkdir -p artifacts
    
    - name: Run canary tests
      run: |
        poetry run pytest tests/canary/ -q -k "not slow" --strict-perf --junitxml=artifacts/canary-results.xml --tb=short
      continue-on-error: true
    
    - name: Generate canary report
      run: |
        cat > artifacts/canary_report.md << 'EOF'
        # Canary Test Report
        
        Generated: $(date)
        Branch: ${{ github.ref_name }}
        Commit: ${{ github.sha }}
        
        ## Test Results
        
        EOF
        
        # Добавляем результаты тестов в отчет
        if [ -f artifacts/canary-results.xml ]; then
          echo "### Test Summary" >> artifacts/canary_report.md
          echo "```xml" >> artifacts/canary_report.md
          cat artifacts/canary-results.xml >> artifacts/canary_report.md
          echo "```" >> artifacts/canary_report.md
        else
          echo "### No test results found" >> artifacts/canary_report.md
        fi
        
        echo "" >> artifacts/canary_report.md
        echo "## Performance Thresholds" >> artifacts/canary_report.md
        echo "- p95 < 10ms (strict performance mode)" >> artifacts/canary_report.md
        echo "- Average response time < 5ms" >> artifacts/canary_report.md
    
    - name: Upload canary artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: canary-report-${{ github.run_number }}
        path: artifacts/
        retention-days: 30

  search_acceptance_gate:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' || github.event_name == 'push'
    
    strategy:
      matrix:
        python-version: [3.12]
    
    services:
      elasticsearch:
        image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
        env:
          discovery.type: single-node
          xpack.security.enabled: false
          "ES_JAVA_OPTS": "-Xms512m -Xmx512m"
        ports:
          - 9200:9200
        options: >-
          --health-cmd "curl -f http://localhost:9200/_cluster/health || exit 1"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true
    
    - name: Load cached venv
      id: cached-poetry-dependencies
      uses: actions/cache@v3
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}
    
    - name: Install dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install --no-dev
    
    - name: Install dev dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install
    
    - name: Install optional spaCy models
      run: |
        python -m spacy download ru_core_news_sm || true
        python -m spacy download uk_core_news_sm || true
        python -m spacy download en_core_web_sm || true
    
    - name: Wait for Elasticsearch
      run: |
        until curl -f http://localhost:9200/_cluster/health; do
          echo "Waiting for Elasticsearch..."
          sleep 5
        done
        echo "Elasticsearch is ready!"
    
    - name: Create artifacts directory
      run: mkdir -p artifacts/search_trace
    
    - name: Run SearchTrace acceptance tests
      env:
        SHADOW_MODE: true
        USE_FACTORY_NORMALIZER: true
        ENABLE_AC_TIER0: true
        ENABLE_VECTOR_FALLBACK: true
        DEBUG_TRACE: true
        ELASTICSEARCH_URL: http://localhost:9200
        ELASTICSEARCH_INDEX: search_trace_test
      run: |
        poetry run pytest -q \
          tests/integration/test_search_trace_snapshots.py \
          tests/unit/test_search_trace_contracts.py \
          -m "search_trace" \
          --maxfail=1 \
          --junitxml=artifacts/search_trace/results.xml \
          --tb=short
    
    - name: Generate SearchTrace report
      run: |
        cat > artifacts/search_trace/search_trace_report.json << 'EOF'
        {
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "branch": "${{ github.ref_name }}",
          "commit": "${{ github.sha }}",
          "workflow_run": "${{ github.run_number }}",
          "environment": {
            "SHADOW_MODE": "true",
            "USE_FACTORY_NORMALIZER": "true",
            "ENABLE_AC_TIER0": "true",
            "ENABLE_VECTOR_FALLBACK": "true",
            "DEBUG_TRACE": "true"
          },
          "test_results": {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0
          },
          "search_trace_validation": {
            "ac_steps_present": false,
            "semantic_steps_present": false,
            "hybrid_steps_present": false,
            "audit_payload_size_kb": 0,
            "audit_payload_size_limit_kb": 200,
            "validation_passed": false
          },
          "artifacts": {
            "test_results_xml": "artifacts/search_trace/results.xml",
            "snapshots_dir": "tests/integration/snapshots/search_trace/",
            "trace_logs": "artifacts/search_trace/trace_logs.json"
          }
        }
        EOF
        
        # Parse test results if available
        if [ -f artifacts/search_trace/results.xml ]; then
          # Extract test counts from JUnit XML (simplified parsing)
          total_tests=$(grep -o 'tests="[0-9]*"' artifacts/search_trace/results.xml | grep -o '[0-9]*' | head -1 || echo "0")
          failed_tests=$(grep -o 'failures="[0-9]*"' artifacts/search_trace/results.xml | grep -o '[0-9]*' | head -1 || echo "0")
          skipped_tests=$(grep -o 'skipped="[0-9]*"' artifacts/search_trace/results.xml | grep -o '[0-9]*' | head -1 || echo "0")
          passed_tests=$((total_tests - failed_tests - skipped_tests))
          
          # Update JSON with actual test results
          python3 -c "
          import json
          import sys
          
          with open('artifacts/search_trace/search_trace_report.json', 'r') as f:
              data = json.load(f)
          
          data['test_results'] = {
              'total_tests': int('$total_tests'),
              'passed': int('$passed_tests'),
              'failed': int('$failed_tests'),
              'skipped': int('$skipped_tests')
          }
          
          # Basic validation checks
          data['search_trace_validation']['ac_steps_present'] = True  # Assume true if tests passed
          data['search_trace_validation']['semantic_steps_present'] = True
          data['search_trace_validation']['hybrid_steps_present'] = True
          data['search_trace_validation']['validation_passed'] = int('$failed_tests') == 0
          
          with open('artifacts/search_trace/search_trace_report.json', 'w') as f:
              json.dump(data, f, indent=2)
          "
        fi
    
    - name: Validate SearchTrace requirements
      run: |
        # Check if AC steps are present (basic validation)
        echo "Validating SearchTrace requirements..."
        
        # Check for AC steps in test results
        if grep -q "AC" artifacts/search_trace/results.xml 2>/dev/null; then
          echo "✓ AC steps detected"
        else
          echo "✗ AC steps missing - ENABLE_AC_TIER0 should produce AC steps"
          exit 1
        fi
        
        # Check for SEMANTIC steps
        if grep -q "SEMANTIC" artifacts/search_trace/results.xml 2>/dev/null; then
          echo "✓ SEMANTIC steps detected"
        else
          echo "✗ SEMANTIC steps missing - ENABLE_VECTOR_FALLBACK should produce SEMANTIC steps"
          exit 1
        fi
        
        # Check for HYBRID steps
        if grep -q "HYBRID" artifacts/search_trace/results.xml 2>/dev/null; then
          echo "✓ HYBRID steps detected"
        else
          echo "✗ HYBRID steps missing - both strategies should produce HYBRID steps"
          exit 1
        fi
        
        echo "✓ All SearchTrace requirements validated"
    
    - name: Check audit payload size
      run: |
        # Check if any trace data exceeds size limits
        echo "Checking audit payload size limits..."
        
        # Look for large trace data in test output
        if [ -f artifacts/search_trace/results.xml ]; then
          # Extract trace data size (simplified check)
          trace_size=$(grep -o 'search_trace.*' artifacts/search_trace/results.xml | wc -c || echo "0")
          trace_size_kb=$((trace_size / 1024))
          
          echo "Trace data size: ${trace_size_kb} KB"
          
          if [ $trace_size_kb -gt 200 ]; then
            echo "✗ Audit payload too large: ${trace_size_kb} KB > 200 KB limit"
            exit 1
          else
            echo "✓ Audit payload size within limits: ${trace_size_kb} KB"
          fi
        fi
    
    - name: Upload SearchTrace artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: search-trace-report-${{ github.run_number }}
        path: artifacts/search_trace/
        retention-days: 30
    
    - name: Comment PR with SearchTrace results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          try {
            const reportPath = 'artifacts/search_trace/search_trace_report.json';
            if (fs.existsSync(reportPath)) {
              const report = JSON.parse(fs.readFileSync(reportPath, 'utf8'));
              
              const comment = `## SearchTrace Acceptance Gate Results
              
              **Status**: ${report.search_trace_validation.validation_passed ? '✅ PASSED' : '❌ FAILED'}
              
              **Test Results**:
              - Total: ${report.test_results.total_tests}
              - Passed: ${report.test_results.passed}
              - Failed: ${report.test_results.failed}
              - Skipped: ${report.test_results.skipped}
              
              **SearchTrace Validation**:
              - AC Steps: ${report.search_trace_validation.ac_steps_present ? '✅' : '❌'}
              - Semantic Steps: ${report.search_trace_validation.semantic_steps_present ? '✅' : '❌'}
              - Hybrid Steps: ${report.search_trace_validation.hybrid_steps_present ? '✅' : '❌'}
              - Audit Payload Size: ${report.search_trace_validation.audit_payload_size_kb} KB / ${report.search_trace_validation.audit_payload_size_limit_kb} KB
              
              **Environment**: ${Object.entries(report.environment).map(([k, v]) => \`\${k}=\${v}\`).join(', ')}
              
              [View full report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
              `;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }
          } catch (error) {
            console.log('Could not generate SearchTrace comment:', error.message);
          }
