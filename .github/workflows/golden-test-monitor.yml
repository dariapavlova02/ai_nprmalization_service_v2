name: Golden Test Monitor

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/ai_service/layers/normalization/**'
      - 'tests/golden_cases/**'
      - 'scripts/ci_golden_monitor.py'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/ai_service/layers/normalization/**'
      - 'tests/golden_cases/**'
      - 'scripts/ci_golden_monitor.py'

jobs:
  golden-test-monitor:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        # Install optional morphology dependencies for testing
        pip install pymorphy3[ru] pymorphy3[uk] || echo "Morphology dependencies optional"

    - name: Run Golden Test Monitor (Strict)
      run: |
        python scripts/ci_golden_monitor.py \
          --min-parity 0.8 \
          --max-p95-latency 50.0 \
          --max-avg-latency 20.0 \
          --min-success-rate 0.95 \
          --output ci_metrics.json \
          --strict

    - name: Upload CI Metrics
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: ci-metrics
        path: ci_metrics.json

    - name: Comment PR with Results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          try {
            const metricsData = JSON.parse(fs.readFileSync('ci_metrics.json', 'utf8'));
            const status = metricsData.build_status === 'pass' ? '‚úÖ PASS' : '‚ùå FAIL';
            const metrics = metricsData.metrics;

            const comment = `## üîç Golden Test Monitor Results ${status}

            ### üìä Metrics Summary
            - **Parity Rate**: ${(metrics.parity_rate * 100).toFixed(1)}% (threshold: 80%)
            - **Factory Accuracy**: ${(metrics.factory_accuracy * 100).toFixed(1)}%
            - **Factory Success Rate**: ${(metrics.factory_success_rate * 100).toFixed(1)}% (threshold: 95%)

            ### ‚è±Ô∏è Performance Metrics
            - **Factory Avg Latency**: ${metrics.factory_avg_latency_ms.toFixed(1)}ms (threshold: 20ms)
            - **Factory P95 Latency**: ${metrics.factory_p95_latency_ms.toFixed(1)}ms (threshold: 50ms)

            ${metricsData.failed_checks.length > 0 ?
              `### ‚ùå Failed Checks\n${metricsData.failed_checks.map(check => `- ${check}`).join('\n')}` :
              '### ‚úÖ All Quality Gates Passed!'}

            <details>
            <summary>View Full Metrics</summary>

            \`\`\`json
            ${JSON.stringify(metricsData, null, 2)}
            \`\`\`

            </details>`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Failed to read metrics file:', error.message);
          }

  performance-regression-check:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    if: github.event_name == 'pull_request'

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need history for comparison

    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .

    - name: Benchmark Current Branch
      run: |
        python scripts/ci_golden_monitor.py \
          --min-parity 0.0 \
          --max-p95-latency 1000 \
          --max-avg-latency 1000 \
          --output current_metrics.json

    - name: Checkout Base Branch
      run: |
        git checkout ${{ github.event.pull_request.base.sha }}
        pip install -e .

    - name: Benchmark Base Branch
      run: |
        python scripts/ci_golden_monitor.py \
          --min-parity 0.0 \
          --max-p95-latency 1000 \
          --max-avg-latency 1000 \
          --output base_metrics.json || echo "Base benchmark failed - skipping regression check"

    - name: Compare Performance
      run: |
        python -c "
        import json
        import sys

        try:
            with open('current_metrics.json') as f:
                current = json.load(f)['metrics']
            with open('base_metrics.json') as f:
                base = json.load(f)['metrics']

            current_p95 = current['factory_p95_latency_ms']
            base_p95 = base['factory_p95_latency_ms']
            regression_threshold = 1.5  # 50% regression threshold

            if current_p95 > base_p95 * regression_threshold:
                print(f'‚ùå Performance regression detected!')
                print(f'Current P95: {current_p95:.1f}ms')
                print(f'Base P95: {base_p95:.1f}ms')
                print(f'Regression: {((current_p95/base_p95-1)*100):.1f}%')
                sys.exit(1)
            else:
                print(f'‚úÖ No significant performance regression')
                print(f'Current P95: {current_p95:.1f}ms vs Base P95: {base_p95:.1f}ms')
        except Exception as e:
            print(f'‚ö†Ô∏è Performance comparison failed: {e}')
            print('Skipping regression check')
        "