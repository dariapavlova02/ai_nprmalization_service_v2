#!/usr/bin/env python3
"""
Aggressive stopwords analysis - extract more garbage terms from payment data
"""

import csv
import re
from collections import defaultdict

def aggressive_stopwords_analysis(filename, top_n=10000):
    """More aggressive analysis to find garbage terms that should be stopwords"""

    # Categories for more aggressive filtering
    definite_stopwords = set()
    potential_stopwords = set()
    business_terms = set()

    # Known name patterns to exclude (so we don't accidentally add names)
    name_patterns = {
        'patronymics': ['–æ–≤–∏—á', '–µ–≤–∏—á', '—ñ–≤–∏—á', '–æ–≤–Ω–∞', '–µ–≤–Ω–∞', '—ñ–≤–Ω–∞'],
        'surnames': ['–µ–Ω–∫–æ', '—É–∫', '—é–∫', '—á—É–∫', '—Å—å–∫–∏–π', '—Å—å–∫–∞', '–∏—á'],
        'common_names': {
            '–æ–ª–µ–∫—Å–∞–Ω–¥—Ä', '–≤–æ–ª–æ–¥–∏–º–∏—Ä', '—Å–µ—Ä–≥—ñ–π', '–∞–Ω–¥—Ä—ñ–π', '–¥–º–∏—Ç—Ä–æ', '–º–∏–∫–æ–ª–∞', '–æ–ª–µ–∫—Å—ñ–π',
            '–≤—ñ–∫—Ç–æ—Ä', '–ø–µ—Ç—Ä–æ', '—ñ–≤–∞–Ω', '—é—Ä—ñ–π', '–º–∏—Ö–∞–π–ª–æ', '—î–≤–≥–µ–Ω', '–≤–∞—Å–∏–ª—å', '–∞–Ω–∞—Ç–æ–ª—ñ–π',
            '–∞–Ω–Ω–∞', '–º–∞—Ä—ñ—è', '–æ–ª–µ–Ω–∞', '–æ–ª—å–≥–∞', '—Ç–µ—Ç—è–Ω–∞', '–Ω–∞—Ç–∞–ª—ñ—è', '—ñ—Ä–∏–Ω–∞', '—é–ª—ñ—è',
            '–∫–∞—Ç–µ—Ä–∏–Ω–∞', '–ª—é–¥–º–∏–ª–∞', '—Å–≤—ñ—Ç–ª–∞–Ω–∞', '–≥–∞–ª–∏–Ω–∞', '–≤–∞–ª–µ–Ω—Ç–∏–Ω–∞'
        }
    }

    # Aggressive business/service term patterns
    business_keywords = [
        # Payment terms
        '–æ–ø–ª–∞—Ç', '–ø–ª–∞—Ç', '–ø–ª–∞—Ç–µ–∂', '–ø–ª–∞—Ç—ñ–∂', '—Å–ø–ª–∞—Ç', '—É–ø–ª–∞—Ç',
        '–ø–æ—Å–ª—É–≥', '—Å–µ—Ä–≤—ñ—Å', '—É—Å–ª—É–≥', '–æ–±—Å–ª—É–≥',
        '–¥–æ–≥–æ–≤–æ—Ä', '–¥–æ–≥–æ–≤—ñ—Ä', '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '—É–≥–æ–¥',
        '–±–∞–ª–∞–Ω—Å', '—Ä–∞—Ö—É–Ω–æ–∫', '—Å—á–µ—Ç', '—Ä–∞—Ö—ñ–≤–Ω',
        '—Ç–∞—Ä–∏—Ñ', '—Ç–∞—Ä–∏—Ñ–Ω', '—Å—Ç–∞–≤–∫',
        '–∫–æ–º—ñ—Å—ñ', '–∫–æ–º–∏—Å—Å—ñ', '–∑–±–æ—Ä',
        '–∑–∞–±–æ—Ä–≥', '–±–æ—Ä–≥', '–¥–æ–ª–≥',

        # Services
        '–∞–±–æ–Ω–µ–Ω—Ç', '–∫–ª—ñ—î–Ω—Ç', '–∫–æ—Ä–∏—Å—Ç—É–≤',
        '–¥–æ—Å—Ç—É–ø', '–ø—ñ–¥–∫–ª—é—á', '–ø–æ–¥–∫–ª—é—á',
        '–Ω–∞–¥–∞–Ω', '–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª', '–Ω–∞–¥–∞–Ω–Ω',
        '–æ–±–ª–∞–¥–Ω–∞–Ω', '–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω', '—Ç–µ—Ö–Ω—ñ—á',

        # Transport
        '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–ø—Ä–æ—ó–∑–¥', '–ø—Ä–æ–µ–∑–¥', '–ø–æ—ó–∑–¥', '–ø–æ–µ–∑–¥',
        '–∫–≤–∏—Ç–æ–∫', '–±–∏–ª–µ—Ç', '–ø—Ä–æ—ó–∑–Ω', '–ø—Ä–æ–µ–∑–¥–Ω',
        '–ø–∞—Ä–∫—É–≤', '—Å—Ç–æ—è–Ω–∫', '–≥–∞—Ä–∞–∂',
        '–∞–≤—Ç–æ–±—É—Å', '—Ç—Ä–æ–ª–µ–π–±—É—Å', '—Ç—Ä–æ–ª–ª–µ–π–±—É—Å', '–º–µ—Ç—Ä–æ',

        # Utilities
        '–µ–ª–µ–∫—Ç—Ä', '–µ–Ω–µ—Ä–≥', '—ç–Ω–µ—Ä–≥', '—Å–≤—ñ—Ç–ª', '–æ—Å–≤—ñ—Ç–ª',
        '–≤–æ–¥–∏', '–≤–æ–¥–Ω', '–≤–æ–¥–æ–ø–æ—Å—Ç', '–≤–æ–¥–æ–≤—ñ–¥',
        '–≥–∞–∑—É', '–≥–∞–∑–æ–≤', '–≥–∞–∑–∏—Ñ—ñ–∫',
        '—Ç–µ–ø–ª–æ', '–æ–ø–∞–ª–µ–Ω', '–æ—Ç–æ–ø–ª–µ–Ω',

        # Documents/Cards
        '–∫–∞—Ä—Ç', '–∫–∞—Ä—Ç–∫', '–∫–∞—Ä—Ç–æ—á',
        '–¥–æ–∫—É–º–µ–Ω—Ç', '–¥–æ–≤—ñ–¥', '—Å–ø—Ä–∞–≤–∫',
        '–ø–∞—Å–ø–æ—Ä—Ç', '–ø–æ—Å–≤—ñ–¥',
        '–Ω–æ–º–µ—Ä', '–∫–æ–¥', '—ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫',

        # Financial
        '–±–∞–Ω–∫', '—Ñ—ñ–Ω–∞–Ω—Å', '–∫—Ä–µ–¥–∏—Ç', '–ø–æ–∑–∏–∫',
        '—Å—Ç—Ä–∞—Ö–æ–≤', '—Å—Ç—Ä–∞—Ö—É–≤', '–ø–æ–ª—ñ—Å',
        '–≤–Ω–µ—Å–∫', '–≤–Ω–µ—Å–æ–∫', '–¥–µ–ø–æ–∑–∏—Ç',
        '–ø—Ä–æ—Ü–µ–Ω—Ç', '–≤—ñ–¥—Å–æ—Ç–∫',

        # Legal/Official
        '–¥–µ—Ä–∂–∞–≤', '—É—Ä—è–¥–æ–≤', '–æ—Ñ—ñ—Ü—ñ–π–Ω',
        '—Ä–µ—î—Å—Ç—Ä', '—Ä–µ–≥—ñ—Å—Ç—Ä', '–æ–±–ª—ñ–∫',
        '–¥–æ–∑–≤—ñ–ª', '–ª—ñ—Ü–µ–Ω–∑', '–ø–∞—Ç–µ–Ω—Ç',
        '—à—Ç—Ä–∞—Ñ', '—Å–∞–Ω–∫—Ü—ñ', '–ø–µ–Ω—è'
    ]

    # Time/date terms
    time_terms = [
        '–¥–µ–Ω—å', '–¥–Ω—è', '–¥–Ω—ñ–≤', '–¥–Ω–µ–π',
        '—Ç–∏–∂–¥–µ–Ω—å', '—Ç–∏–∂–Ω', '–Ω–µ–¥–µ–ª',
        '–º—ñ—Å—è—Ü—å', '–º–µ—Å—è—Ü', '–º—ñ—Å',
        '—Ä—ñ–∫', '—Ä–æ–∫—É', '—Ä–æ–∫—ñ–≤', '–≥–æ–¥', '–ª–µ—Ç',
        '–≥–æ–¥–∏–Ω–∞', '–≥–æ–¥–∏–Ω', '—á–∞—Å',
        '—Ö–≤–∏–ª–∏–Ω', '–º–∏–Ω—É—Ç',
        '—Å—ñ—á–µ–Ω—å', '–ª—é—Ç–∏–π', '–±–µ—Ä–µ–∑–µ–Ω—å', '–∫–≤—ñ—Ç–µ–Ω—å', '—Ç—Ä–∞–≤–µ–Ω—å', '—á–µ—Ä–≤–µ–Ω—å',
        '–ª–∏–ø–µ–Ω—å', '—Å–µ—Ä–ø–µ–Ω—å', '–≤–µ—Ä–µ—Å–µ–Ω—å', '–∂–æ–≤—Ç–µ–Ω—å', '–ª–∏—Å—Ç–æ–ø–∞–¥', '–≥—Ä—É–¥–µ–Ω—å',
        '—è–Ω–≤–∞—Ä—å', '—Ñ–µ–≤—Ä–∞–ª—å', '–º–∞—Ä—Ç', '–∞–ø—Ä–µ–ª—å', '–º–∞–π', '–∏—é–Ω—å',
        '–∏—é–ª—å', '–∞–≤–≥—É—Å—Ç', '—Å–µ–Ω—Ç—è–±—Ä—å', '–æ–∫—Ç—è–±—Ä—å', '–Ω–æ—è–±—Ä—å', '–¥–µ–∫–∞–±—Ä—å'
    ]

    # Location terms (cities, regions, addresses)
    location_terms = [
        '–∫–∏—ó–≤', '–æ–¥–µ—Å–∞', '—Ö–∞—Ä–∫—ñ–≤', '–¥–Ω—ñ–ø—Ä–æ', '–ª—å–≤—ñ–≤', '–∑–∞–ø–æ—Ä—ñ–∂–∂—è',
        '–∫—Ä–∏–≤–∏–π', '–º–∏–∫–æ–ª–∞—ó–≤', '–º–∞—Ä—ñ—É–ø–æ–ª—å', '–ª—É–≥–∞–Ω—Å—å–∫', '—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å',
        '–≤—ñ–Ω–Ω–∏—Ü—è', '—á–µ—Ä–Ω—ñ–≥—ñ–≤', '—á–µ—Ä–∫–∞—Å–∏', '–∂–∏—Ç–æ–º–∏—Ä', '—Å—É–º–∏', '—Ö–º–µ–ª—å–Ω–∏—Ü',
        '—á–µ—Ä–Ω—ñ–≤—Ü—ñ', '—Ä—ñ–≤–Ω–µ', '–∫—Ä–µ–º–µ–Ω—á—É–≥', '–∫—Ä–æ–ø–∏–≤–Ω–∏—Ü', '—ñ–≤–∞–Ω–æ', '—Ç–µ—Ä–Ω–æ–ø—ñ–ª—å',
        '–ª—É—Ü—å–∫', '—É–∂–≥–æ—Ä–æ–¥', '–±—ñ–ª–∞', '–º–µ–ª—ñ—Ç–æ–ø–æ–ª—å', '–∫–µ—Ä—á', '–±–µ—Ä–¥—è–Ω—Å—å–∫',
        '–Ω—ñ–∫–æ–ø–æ–ª—å', '—Å–ª–æ–≤', '–∫—Ä–∞–º–∞—Ç–æ—Ä—Å—å–∫', '–∫–æ–Ω–æ—Ç–æ–ø', '—É–º–∞–Ω—å', '–±—Ä–æ–≤–∞—Ä–∏',
        '–º—É–∫–∞—á–µ–≤–æ', '–∫–æ–ª–æ–º–∏—è', '—î–≤–ø–∞—Ç–æ—Ä—ñ—è', '—è–ª—Ç–∞', '–∞–ª—É—à—Ç–∞', '—Ñ–µ–æ–¥–æ—Å—ñ—è',

        '–æ–±–ª–∞—Å—Ç—å', '–æ–±–ª–∞—Å–Ω', '—Ä–∞–π–æ–Ω', '—Ä–∞–π–æ–Ω–Ω', '–º—ñ—Å—Ç–æ', '–≥–æ—Ä–æ–¥',
        '—Å–µ–ª–æ', '—Å–µ–ª–∏—â', '–ø–æ—Å–µ–ª–æ–∫', '—Å—Ç–∞–Ω—Ü—ñ—è', '–≤–æ–∫–∑–∞–ª',
        '–≤—É–ª–∏—Ü—è', '—É–ª–∏—Ü–∞', '–ø—Ä–æ—Å–ø–µ–∫—Ç', '–±—É–ª—å–≤–∞—Ä', '–ø–ª–æ—â–∞', '–ø–ª–æ—â–∞–¥',
        '–ø—Ä–æ–≤—É–ª–æ–∫', '–ø–µ—Ä–µ—É–ª–æ–∫', '–Ω–∞–±–µ—Ä–µ–∂–Ω–∞', '–Ω–∞–±–µ—Ä–µ–∂–Ω–∞—è'
    ]

    print(f"üîç Aggressive stopwords analysis of top {top_n} tokens...")

    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file)

        for i, row in enumerate(reader):
            if i >= top_n:
                break

            token = row['–¢–æ–∫–µ–Ω'].lower().strip()
            frequency = int(row['–ß–∞—Å—Ç–æ—Ç–∞'])
            percent = float(row['–ü—Ä–æ—Ü–µ–Ω—Ç'].replace('%', ''))

            # Skip very short tokens or numbers
            if len(token) < 3 or any(char.isdigit() for char in token):
                continue

            # Skip if it looks like a name
            is_likely_name = False

            # Check if it's a known name
            if token in name_patterns['common_names']:
                continue

            # Check patronymic patterns
            if any(token.endswith(pattern) for pattern in name_patterns['patronymics']):
                continue

            # Check surname patterns
            if any(token.endswith(pattern) for pattern in name_patterns['surnames']):
                continue

            # Check if it matches business patterns
            business_score = 0
            for keyword in business_keywords:
                if keyword in token:
                    business_score += 1
                    break

            # Check time terms
            time_score = 0
            for term in time_terms:
                if term in token or token in term:
                    time_score += 1
                    break

            # Check location terms
            location_score = 0
            for term in location_terms:
                if term in token or token in term:
                    location_score += 1
                    break

            # Decision logic
            total_score = business_score + time_score + location_score

            # High frequency terms are more likely to be garbage
            frequency_bonus = 0
            if frequency > 10000:  # >0.14%
                frequency_bonus = 2
            elif frequency > 5000:   # >0.07%
                frequency_bonus = 1

            final_score = total_score + frequency_bonus

            # Categorize
            if final_score >= 2:
                definite_stopwords.add(token)
                print(f"‚úÖ DEFINITE: {token} (freq: {frequency}, {percent:.3f}%, score: {final_score})")
            elif final_score == 1 and frequency > 1000:
                potential_stopwords.add(token)
                print(f"‚ùì POTENTIAL: {token} (freq: {frequency}, {percent:.3f}%, score: {final_score})")

            # Additional check for very high frequency non-name terms
            if (frequency > 5000 and
                len(token) >= 4 and
                not any(token.endswith(pattern) for pattern in name_patterns['patronymics']) and
                not any(token.endswith(pattern) for pattern in name_patterns['surnames']) and
                token not in name_patterns['common_names']):

                # Check if it could be a business term by content
                if (any(substring in token for substring in ['–æ–ø–ª–∞—Ç', '–ø–æ—Å–ª—É–≥', '–¥–æ–≥–æ–≤–æ—Ä', '–∫–∞—Ä—Ç', '–±–∞–Ω–∫', '—Å–µ—Ä–≤—ñ—Å', '—Ç–∞—Ä–∏—Ñ']) or
                    token in time_terms or
                    any(loc_term in token for loc_term in location_terms[:20])): # Top cities only

                    business_terms.add(token)
                    print(f"üíº BUSINESS: {token} (freq: {frequency}, {percent:.3f}%)")

    return definite_stopwords, potential_stopwords, business_terms

def generate_comprehensive_additions(definite_stopwords, potential_stopwords, business_terms):
    """Generate comprehensive stopwords additions"""

    print(f"\n" + "="*80)
    print(f"üéØ COMPREHENSIVE STOPWORDS ANALYSIS")
    print(f"="*80)

    print(f"\nüìä STATISTICS:")
    print(f"  ‚Ä¢ Definite stopwords: {len(definite_stopwords)}")
    print(f"  ‚Ä¢ Potential stopwords: {len(potential_stopwords)}")
    print(f"  ‚Ä¢ Business terms: {len(business_terms)}")
    print(f"  ‚Ä¢ TOTAL RECOMMENDED: {len(definite_stopwords) + len(potential_stopwords) + len(business_terms)}")

    all_recommendations = definite_stopwords.union(potential_stopwords).union(business_terms)

    print(f"\nüîß RECOMMENDED ADDITIONS TO stopwords.py:")
    print(f"Add these {len(all_recommendations)} terms to STOP_ALL:")
    print()

    # Format for easy copy-paste
    sorted_terms = sorted(list(all_recommendations))
    for i in range(0, len(sorted_terms), 6):
        batch = sorted_terms[i:i+6]
        formatted = [f'"{term}"' for term in batch]
        print("    " + ", ".join(formatted) + ",")

    print(f"\nüí° IMPACT ANALYSIS:")
    print(f"  ‚Ä¢ This will filter out {len(all_recommendations)} high-frequency garbage terms")
    print(f"  ‚Ä¢ Should significantly improve name detection precision")
    print(f"  ‚Ä¢ Covers payment, service, location, and temporal terms")
    print(f"  ‚Ä¢ Preserves all identified person names")

if __name__ == "__main__":
    definite, potential, business = aggressive_stopwords_analysis("all_tokens_by_frequency.csv", top_n=5000)
    generate_comprehensive_additions(definite, potential, business)