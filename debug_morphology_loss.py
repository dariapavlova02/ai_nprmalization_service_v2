#!/usr/bin/env python3
"""
–û—Ç–ª–∞–¥–∫–∞ –ø–æ—Ç–µ—Ä–∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ.
"""

import os
import sys
from pathlib import Path

# Set production environment variables + extra logging
os.environ.update({
    'PRESERVE_FEMININE_SURNAMES': 'true',
    'ENABLE_ENHANCED_GENDER_RULES': 'true',
    'PRESERVE_FEMININE_SUFFIX_UK': 'true',
    'ENABLE_FSM_TUNED_ROLES': 'true',
    'MORPHOLOGY_CUSTOM_RULES_FIRST': 'true',
    'ENABLE_ADVANCED_FEATURES': 'true',
    'NORMALIZATION_ENABLE_ADVANCED_FEATURES': 'true',
    'NORMALIZATION_ENABLE_MORPHOLOGY': 'true',
    'DEBUG_TRACING': 'true'
})

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def patch_factory_with_logging():
    """–ü–∞—Ç—á–∏–º NormalizationFactory –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
    from ai_service.layers.normalization.processors.normalization_factory import NormalizationFactory

    original_normalize = NormalizationFactory.normalize_async

    async def debug_normalize(self, text, config):
        print(f"\nüîç –ù–ê–ß–ê–õ–û –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò: '{text}'")

        # Call original method but intercept intermediate results
        result = await original_normalize(text, config)

        print(f"üîç –§–ò–ù–ê–õ–¨–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
        print(f"   normalized: '{result.normalized}'")
        print(f"   tokens: {result.tokens}")

        return result

    # Monkey patch
    NormalizationFactory.normalize_async = debug_normalize

    # Also patch morphology method to track intermediate results
    original_normalize_morphology = NormalizationFactory._normalize_morphology

    async def debug_normalize_morphology(self, tokens, roles, config, skip_indices=None, effective_flags=None):
        print(f"\nüîç –í–•–û–î –í –ú–û–†–§–û–õ–û–ì–ò–Æ:")
        print(f"   input tokens: {tokens}")
        print(f"   input roles: {roles}")

        normalized_tokens, morph_traces = await original_normalize_morphology(tokens, roles, config, skip_indices, effective_flags)

        print(f"üîç –í–´–•–û–î –ò–ó –ú–û–†–§–û–õ–û–ì–ò–ò:")
        print(f"   output tokens: {normalized_tokens}")
        print(f"   morph traces: {morph_traces[:3]}")  # First 3 traces

        return normalized_tokens, morph_traces

    NormalizationFactory._normalize_morphology = debug_normalize_morphology

def test_morphology_tracing():
    """–¢–µ—Å—Ç —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏."""
    print("üîç –û–¢–°–õ–ï–ñ–ò–í–ê–ù–ò–ï –ü–û–¢–ï–†–ò –ú–û–†–§–û–õ–û–ì–ò–ß–ï–°–ö–ò–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*70)

    try:
        # Patch factory first
        patch_factory_with_logging()

        from ai_service.layers.normalization.normalization_service import NormalizationService

        service = NormalizationService()

        text = "–ü–æ—Ä–æ—à–µ–Ω–∫–∞ –ü–µ—Ç–µ–Ω—å–∫–∞"
        print(f"üìù –¢–µ—Å—Ç: '{text}'")

        result = service.normalize_sync(
            text=text,
            language="uk",
            remove_stop_words=True,
            preserve_names=True,
            enable_advanced_features=True
        )

        print(f"\n‚úÖ –ò–¢–û–ì–û–í–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
        print(f"   Normalized: '{result.normalized}'")
        print(f"   Tokens: {result.tokens}")

        if result.normalized != "–ü–æ—Ä–æ—à–µ–Ω–∫–æ –ü–µ—Ç—Ä–æ":
            print(f"‚ùå –û–®–ò–ë–ö–ê! –û–∂–∏–¥–∞–ª–∏: '–ü–æ—Ä–æ—à–µ–Ω–∫–æ –ü–µ—Ç—Ä–æ'")
        else:
            print(f"‚úÖ –£–°–ü–ï–•! –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_morphology_tracing()