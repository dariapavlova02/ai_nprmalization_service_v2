#!/usr/bin/env python3
"""
Test AND–†–Ü–ô in final normalized text after FSM fix
"""

import sys
import json
sys.path.insert(0, '.')
sys.path.insert(0, 'src')

def test_andrii_final():
    """Test AND–†–Ü–ô in final result."""
    print("üîç TESTING –ê–ù–î–†–Ü–ô IN FINAL RESULT")
    print("="*50)

    try:
        from ai_service.layers.normalization.normalization_service import NormalizationService

        service = NormalizationService()

        # Test case
        test_input = "–®–ï–í–ß–ï–ù–ö–û –ê–ù–î–†–Ü–ô –ê–ù–ê–¢–û–õ–Ü–ô–û–í–ò–ß"
        print(f"Input: '{test_input}'")

        result = service.normalize_sync(
            test_input,
            language="uk",
            remove_stop_words=True,
            preserve_names=True,
            enable_advanced_features=True
        )

        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"  normalized_text: '{result.normalized}'")
        print(f"  tokens: {result.tokens}")
        print(f"  persons_core: {result.persons_core}")
        print(f"  success: {result.success}")

        # Check –ê–ù–î–†–Ü–ô presence
        print(f"\n‚úÖ –ü–†–û–í–ï–†–ö–ò:")
        if "–ê–Ω–¥—Ä—ñ–π" in result.normalized or "–∞–Ω–¥—Ä—ñ–π" in result.normalized.lower():
            print("  ‚úÖ –ê–ù–î–†–Ü–ô –Ω–∞–π–¥–µ–Ω –≤ normalized_text!")
        else:
            print("  ‚ùå –ê–ù–î–†–Ü–ô –ù–ï –Ω–∞–π–¥–µ–Ω –≤ normalized_text!")

        # Check for duplicates in persons_core
        if result.persons_core:
            all_tokens = []
            for person_tokens in result.persons_core:
                all_tokens.extend(person_tokens)

            duplicates = []
            seen = set()
            for token in all_tokens:
                token_lower = token.lower()
                if token_lower in seen:
                    duplicates.append(token)
                seen.add(token_lower)

            if duplicates:
                print(f"  ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ persons_core: {duplicates}")
            else:
                print(f"  ‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ persons_core –Ω–µ—Ç")

        # Detailed trace for –ê–ù–î–†–Ü–ô
        print(f"\nüìù TRACE –¥–ª—è –ê–ù–î–†–Ü–ô:")
        andrii_traces = [t for t in result.trace if '–∞–Ω–¥—Ä—ñ–π' in t.token.lower()]
        for i, trace in enumerate(andrii_traces):
            print(f"  {i+1}. Token: '{trace.token}' -> Role: '{trace.role}' -> Output: '{trace.output}'")

        # Show unique tokens in trace
        print(f"\nüìä –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ trace:")
        unique_tokens = set()
        for trace in result.trace:
            if hasattr(trace, 'token'):
                unique_tokens.add(trace.token)
        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {unique_tokens}")

        # Count duplicates in trace
        token_counts = {}
        for trace in result.trace:
            if hasattr(trace, 'token'):
                token = trace.token
                token_counts[token] = token_counts.get(token, 0) + 1

        duplicated_tokens = {k: v for k, v in token_counts.items() if v > 1}
        if duplicated_tokens:
            print(f"\n‚ö†Ô∏è –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ trace:")
            for token, count in duplicated_tokens.items():
                print(f"  '{token}': {count} —Ä–∞–∑")

    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_andrii_final()