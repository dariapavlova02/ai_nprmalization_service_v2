#!/usr/bin/env python3
"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è AI Service Normalization.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É LRU –∫—ç—à–µ–π —Å TTL, –º–µ—Ç—Ä–∏–∫–∏ Prometheus
–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö.
"""

import asyncio
import sys
import time
import statistics
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from ai_service.utils.lru_cache_ttl import LruTtlCache, CacheManager, create_flags_hash
from ai_service.layers.normalization.tokenizer_service import TokenizerService, CachedTokenizerService
from ai_service.layers.normalization.morphology_adapter import MorphologyAdapter
from ai_service.monitoring.cache_metrics import CacheMetrics, MetricsCollector
from ai_service.layers.normalization.processors.normalization_factory import NormalizationFactory, NormalizationConfig


# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
TEST_TEXTS = [
    "–Ü–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤",
    "–û–û–û '–†–æ–º–∞—à–∫–∞' –ò–≤–∞–Ω –ò.",
    "–ü–µ—Ç—Ä–æ –ü–æ—Ä–æ—à–µ–Ω–∫–æ",
    "John Smith",
    "–ê–Ω–Ω–∞ –°–µ—Ä–≥–µ–µ–≤–Ω–∞ –ò–≤–∞–Ω–æ–≤–∞",
    "Dr. John Smith",
    "Prof. Maria Garcia",
    "Mr. –ü–µ—Ç—Ä –ü–µ—Ç—Ä–æ–≤",
    "Ms. –ê–Ω–Ω–∞ –ò–≤–∞–Ω–æ–≤–∞",
    "–Ü–≤–∞–Ω I. –ü–µ—Ç—Ä–æ–≤"
]


def demo_lru_cache():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è LRU –∫—ç—à–∞ —Å TTL."""
    print("üîß –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è LRU –∫—ç—à–∞ —Å TTL")
    print("-" * 50)
    
    # –°–æ–∑–¥–∞—ë–º –∫—ç—à
    cache = LruTtlCache(maxsize=5, ttl_seconds=2)
    
    print("1. –ë–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:")
    cache.set("key1", "value1")
    hit, value = cache.get("key1")
    print(f"   key1: hit={hit}, value={value}")
    
    hit, value = cache.get("nonexistent")
    print(f"   nonexistent: hit={hit}, value={value}")
    
    print("\n2. LRU eviction:")
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –∫—ç—à
    for i in range(7):
        cache.set(f"key{i}", f"value{i}")
    
    print(f"   –†–∞–∑–º–µ—Ä –∫—ç—à–∞: {len(cache)}")
    print(f"   key0 –¥–æ—Å—Ç—É–ø–µ–Ω: {'key0' in cache}")
    print(f"   key6 –¥–æ—Å—Ç—É–ø–µ–Ω: {'key6' in cache}")
    
    print("\n3. TTL expiration:")
    cache.set("temp", "temporary")
    hit, _ = cache.get("temp")
    print(f"   temp —Å—Ä–∞–∑—É: hit={hit}")
    
    time.sleep(2.1)  # –ñ–¥—ë–º –∏—Å—Ç–µ—á–µ–Ω–∏—è TTL
    hit, _ = cache.get("temp")
    print(f"   temp –ø–æ—Å–ª–µ TTL: hit={hit}")
    
    print("\n4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:")
    stats = cache.get_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")


def demo_cache_manager():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫—ç—à–µ–π."""
    print("\nüèóÔ∏è –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫—ç—à–µ–π")
    print("-" * 50)
    
    config = {
        'max_size': 100,
        'ttl_sec': 60,
        'enable_cache': True
    }
    
    manager = CacheManager(config)
    
    print("1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"   maxsize: {manager.maxsize}")
    print(f"   ttl_seconds: {manager.ttl_seconds}")
    print(f"   enabled: {manager.enabled}")
    
    print("\n2. –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∫—ç—à–µ–π:")
    tokenizer_cache = manager.get_tokenizer_cache()
    morphology_cache = manager.get_morphology_cache()
    
    for i in range(10):
        tokenizer_cache.set(f"token_{i}", f"tokens_{i}")
        morphology_cache.set(f"morph_{i}", f"morphs_{i}")
    
    print(f"   Tokenizer cache size: {len(tokenizer_cache)}")
    print(f"   Morphology cache size: {len(morphology_cache)}")
    
    print("\n3. –û–±—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:")
    all_stats = manager.get_all_stats()
    for layer, stats in all_stats.items():
        if layer != 'config':
            print(f"   {layer}: size={stats['size']}, hits={stats['hits']}, misses={stats['misses']}")


def demo_tokenizer_service():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    print("\nüî§ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏")
    print("-" * 50)
    
    # –°–æ–∑–¥–∞—ë–º –∫—ç—à –∏ —Å–µ—Ä–≤–∏—Å
    cache = LruTtlCache(maxsize=50, ttl_seconds=60)
    service = TokenizerService(cache)
    
    print("1. –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ (cache miss):")
    result1 = service.tokenize("–Ü–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤", language="uk")
    print(f"   –¢–æ–∫–µ–Ω—ã: {result1.tokens}")
    print(f"   Cache hit: {result1.cache_hit}")
    print(f"   –í—Ä–µ–º—è: {result1.processing_time:.4f}s")
    
    print("\n2. –í—Ç–æ—Ä–æ–π –≤—ã–∑–æ–≤ (cache hit):")
    result2 = service.tokenize("–Ü–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤", language="uk")
    print(f"   –¢–æ–∫–µ–Ω—ã: {result2.tokens}")
    print(f"   Cache hit: {result2.cache_hit}")
    print(f"   –í—Ä–µ–º—è: {result2.processing_time:.4f}s")
    
    print("\n3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–∏—Å–∞:")
    stats = service.get_stats()
    for key, value in stats.items():
        if key != 'cache':
            print(f"   {key}: {value}")
    
    if 'cache' in stats:
        cache_stats = stats['cache']
        print(f"   Cache hit rate: {cache_stats['hit_rate']:.2f}%")


def demo_morphology_adapter():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    print("\nüîç –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏")
    print("-" * 50)
    
    # –°–æ–∑–¥–∞—ë–º –∫—ç—à –∏ –∞–¥–∞–ø—Ç–µ—Ä
    cache = LruTtlCache(maxsize=50, ttl_seconds=60)
    adapter = MorphologyAdapter(cache_size=1000, cache=cache)
    
    print("1. –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ (cache miss):")
    result1 = adapter.parse("–ü–µ—Ç—Ä–æ–≤", "ru")
    print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {result1}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤: {len(result1)}")
    print(f"   –í—Ä–µ–º—è: {time.perf_counter() - time.perf_counter():.4f}s")
    
    print("\n2. –í—Ç–æ—Ä–æ–π –≤—ã–∑–æ–≤ (cache hit):")
    result2 = adapter.parse("–ü–µ—Ç—Ä–æ–≤", "ru")
    print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {result2}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤: {len(result2)}")
    print(f"   –í—Ä–µ–º—è: {time.perf_counter() - time.perf_counter():.4f}s")
    
    print("\n3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞–¥–∞–ø—Ç–µ—Ä–∞:")
    stats = adapter.get_cache_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    if 'cache' in stats:
        cache_stats = stats['cache']
        print(f"   Cache hit rate: {cache_stats['hit_rate']:.2f}%")


def demo_metrics():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ Prometheus."""
    print("\nüìä –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ Prometheus")
    print("-" * 50)
    
    metrics = CacheMetrics()
    
    print("1. –ó–∞–ø–∏—Å—å —Å–æ–±—ã—Ç–∏–π –∫—ç—à–∞:")
    metrics.record_tokenizer_cache_hit("uk")
    metrics.record_tokenizer_cache_miss("uk")
    metrics.record_morphology_cache_hit("ru")
    metrics.record_morphology_cache_miss("ru")
    
    print("2. –ó–∞–ø–∏—Å—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏:")
    metrics.record_layer_latency("tokenizer", "uk", 0.01)
    metrics.record_layer_latency("morphology", "ru", 0.005)
    metrics.record_normalization_latency("uk", 0.05)
    
    print("3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫—ç—à–µ–π:")
    metrics.update_tokenizer_cache_size("uk", 100)
    metrics.update_morphology_cache_size("ru", 50)
    
    print("4. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ hit rate:")
    metrics.update_tokenizer_cache_hit_rate("uk", 75.5)
    metrics.update_morphology_cache_hit_rate("ru", 80.0)
    
    print("   –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ Prometheus registry")


async def demo_performance():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    print("\n‚ö° –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    print("-" * 50)
    
    # –°–æ–∑–¥–∞—ë–º factory —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    cache_manager = CacheManager({
        'max_size': 1000,
        'ttl_sec': 300,
        'enable_cache': True
    })
    
    factory = NormalizationFactory(cache_manager=cache_manager)
    
    config = NormalizationConfig(
        enable_cache=True,
        debug_tracing=True,
        language="auto"
    )
    
    print("1. –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ (–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∫—ç—à–∞):")
    latencies = []
    for text in TEST_TEXTS:
        start_time = time.perf_counter()
        result = await factory.normalize_text(text, config)
        end_time = time.perf_counter()
        
        latency = end_time - start_time
        latencies.append(latency)
        
        print(f"   '{text}' -> '{result.normalized}' ({latency*1000:.2f}ms)")
    
    print(f"\n   –°—Ä–µ–¥–Ω—è—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {statistics.mean(latencies)*1000:.2f}ms")
    
    print("\n2. –í—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞):")
    latencies_cached = []
    for text in TEST_TEXTS:
        start_time = time.perf_counter()
        result = await factory.normalize_text(text, config)
        end_time = time.perf_counter()
        
        latency = end_time - start_time
        latencies_cached.append(latency)
        
        print(f"   '{text}' -> '{result.normalized}' ({latency*1000:.2f}ms)")
    
    print(f"\n   –°—Ä–µ–¥–Ω—è—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (–∫—ç—à): {statistics.mean(latencies_cached)*1000:.2f}ms")
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —É–ª—É—á—à–µ–Ω–∏–µ
    improvement = (statistics.mean(latencies) - statistics.mean(latencies_cached)) / statistics.mean(latencies) * 100
    print(f"   –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:.1f}%")
    
    print("\n3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–µ–π:")
    all_stats = cache_manager.get_all_stats()
    for layer, stats in all_stats.items():
        if layer != 'config':
            print(f"   {layer}:")
            print(f"     Size: {stats['size']}")
            print(f"     Hit rate: {stats['hit_rate']:.2f}%")
            print(f"     Hits: {stats['hits']}")
            print(f"     Misses: {stats['misses']}")


def demo_cache_key_generation():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–µ–π –∫—ç—à–∞."""
    print("\nüîë –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–µ–π –∫—ç—à–∞")
    print("-" * 50)
    
    print("1. –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–µ–π –∫—ç—à–∞:")
    key1 = ("uk", "–Ü–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤", "abc123def4")
    print(f"   –ö–ª—é—á: {key1}")
    
    print("\n2. –°–æ–∑–¥–∞–Ω–∏–µ —Ö—ç—à–∞ —Ñ–ª–∞–≥–æ–≤:")
    flags1 = {"remove_stop_words": True, "preserve_names": True}
    flags2 = {"preserve_names": True, "remove_stop_words": True}  # –î—Ä—É–≥–æ–π –ø–æ—Ä—è–¥–æ–∫
    
    hash1 = create_flags_hash(flags1)
    hash2 = create_flags_hash(flags2)
    
    print(f"   –§–ª–∞–≥–∏ 1: {flags1}")
    print(f"   –•—ç—à 1: {hash1}")
    print(f"   –§–ª–∞–≥–∏ 2: {flags2}")
    print(f"   –•—ç—à 2: {hash2}")
    print(f"   –•—ç—à–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã: {hash1 == hash2}")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏."""
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è AI Service")
    print("=" * 60)
    
    try:
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        demo_lru_cache()
        demo_cache_manager()
        demo_tokenizer_service()
        demo_morphology_adapter()
        demo_metrics()
        demo_cache_key_generation()
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        await demo_performance()
        
        print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print("\n–°–∏—Å—Ç–µ–º–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:")
        print("- LRU –∫—ç—à–∏ —Å TTL –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏")
        print("- Prometheus –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
        print("- Debug tracing —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫—ç—à–µ")
        print("- –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å p95 ‚â§ 10ms –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
