#!/usr/bin/env python3
"""
Analyze payment tokens and categorize for stopwords and diminutives
"""

import csv
import re
from collections import defaultdict

def analyze_tokens(filename, top_n=1000):
    """Analyze top N tokens from payment descriptions"""

    # Categories for classification
    categories = {
        'payment_words': set(),        # Payment/service related
        'numbers_dates': set(),        # Numbers, dates, codes
        'service_names': set(),        # Company/service names
        'locations': set(),           # Cities, addresses
        'technical_terms': set(),     # Technical/system terms
        'potential_names': set(),     # Potential first names
        'potential_patronymics': set(), # Potential patronymics
        'potential_surnames': set(),  # Potential surnames
        'garbage_tokens': set(),      # Short/meaningless tokens
        'already_in_stopwords': set() # Already covered
    }

    # Existing stopwords for comparison
    existing_stopwords = {
        '–æ–ø–ª–∞—Ç–∞', '–∑–∞', '–ø–æ—Å–ª—É–≥', '–Ω–∞', '–ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è', '–ø–æ—Å–ª—É–≥–∏', '–±–∞–ª–∞–Ω—Å—É', '—Å–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–∞',
        '–¥–æ–≥–æ–≤–æ—Ä—É', '–¥–æ–≥–æ–≤–æ—Ä', '—Ä–∞—Ö—É–Ω–æ–∫', '—Ä–∞—Ö—É–Ω–∫—É', '–ø–ª–∞—Ç—ñ–∂', '–ø–ª–∞—Ç–µ–∂', '–¥–æ—Å—Ç—É–ø', '–∫–ª—ñ—î–Ω—Ç–∞',
        '—Ç–æ–≤', '–∑–∞–±–æ—Ä–≥–æ–≤–∞–Ω—ñ—Å—Ç—å', '—Å—É–º–∞', '—Å—É–º–∏', '—Å—É–º–º–∞', '—á–µ—Ä–µ–∑', '–∫–æ–º–ø–∞–Ω—ñ—ó', '–¥–æ—Å—Ç–∞–≤–∫—É',
        '–ø–ª–∞—Ç–Ω–∏–∫', '–∫–æ–º—ñ—Å—ñ—ó', '–Ω–æ–º–µ—Ä', '–∫–∞—Ä—Ç—É', '–¥–ª—è', '–ø–æ', '–≤—ñ–¥', '—Ç–∞', '—ñ', '—ñ', '—É', '–≤', '–∑'
    }

    # Name patterns
    name_endings_m = ['–æ–≤–∏—á', '–µ–≤–∏—á', '—ñ–≤–∏—á', '–π–æ–≤–∏—á', '–ª—å–æ–≤–∏—á', '—Ä–æ–≤–∏—á', '—Å–æ–≤–∏—á', '—Ç–æ–≤–∏—á']
    name_endings_f = ['—ñ–≤–Ω–∞', '–æ–≤–Ω–∞', '–µ–≤–Ω–∞', '–π—ñ–≤–Ω–∞', '–ª—å—ñ–≤–Ω–∞', '—Ä—ñ–≤–Ω–∞', '—Å—ñ–≤–Ω–∞', '—Ç—ñ–≤–Ω–∞']
    surname_endings = ['–µ–Ω–∫–æ', '—É–∫', '—é–∫', '—á—É–∫', '—Å—å–∫–∏–π', '—Å—å–∫–∞', '—Ü—å–∫–∏–π', '—Ü—å–∫–∞', '–∏—á', '–æ–≤–∏—á', '–∫–æ', '–µ–Ω–∫–æ']

    # Service/company indicators
    service_indicators = ['–≥–∞–∑', '–µ–ª–µ–∫—Ç—Ä', '–≤–æ–¥', '—Ç–µ–ø–ª–æ', '—ñ–Ω—Ç–µ—Ä–Ω–µ—Ç', '—Ç–µ–ª–µ–∫–æ–º', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–±–∞–Ω–∫']

    print(f"üîç Analyzing top {top_n} payment tokens...")

    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file)

        for i, row in enumerate(reader):
            if i >= top_n:
                break

            token = row['–¢–æ–∫–µ–Ω'].lower().strip()
            frequency = int(row['–ß–∞—Å—Ç–æ—Ç–∞'])
            percent = float(row['–ü—Ä–æ—Ü–µ–Ω—Ç'].replace('%', ''))

            # Skip very short tokens (likely fragments)
            if len(token) <= 2 and not token.isdigit():
                categories['garbage_tokens'].add(token)
                continue

            # Already in stopwords
            if token in existing_stopwords:
                categories['already_in_stopwords'].add(token)
                continue

            # Numbers and dates
            if token.isdigit() or re.match(r'\d+', token):
                categories['numbers_dates'].add(token)
                continue

            # Payment/service words (high frequency business terms)
            if any(indicator in token for indicator in ['–æ–ø–ª–∞—Ç', '–ø–ª–∞—Ç', '–ø–æ—Å–ª—É–≥', '—Å–µ—Ä–≤—ñ—Å', '–±–∞–ª–∞–Ω—Å', '—Ä–∞—Ö—É–Ω–æ–∫', '–¥–æ–≥–æ–≤—ñ—Ä', '–¥–æ–≥–æ–≤–æ—Ä', '–∞–±–æ–Ω–µ–Ω—Ç', '—Ç–∞—Ä–∏—Ñ', '–∫–æ–º—ñ—Å—ñ', '–∑–∞–±–æ—Ä–≥']):
                categories['payment_words'].add(token)
                continue

            # Technical/service names
            if any(indicator in token for indicator in service_indicators):
                categories['service_names'].add(token)
                continue

            # Location indicators (cities, regions)
            if any(indicator in token for indicator in ['–∫–∏—ó–≤', '–æ–¥–µ—Å', '—Ö–∞—Ä–∫—ñ–≤', '–ª—å–≤—ñ–≤', '–¥–Ω—ñ–ø—Ä', '–∑–∞–ø–æ—Ä—ñ–∂', '–∫—Ä–µ–º–µ–Ω', '–ª—É—Ü—å–∫', '—É–∂–≥–æ—Ä–æ–¥', '—á–µ—Ä–Ω—ñ–≥—ñ–≤', '–º—É–∫–∞—á—ñ–≤']):
                categories['locations'].add(token)
                continue

            # Potential patronymics
            if any(token.endswith(ending) for ending in name_endings_m + name_endings_f):
                if len(token) > 6:  # Reasonable length for patronymic
                    categories['potential_patronymics'].add(token)
                continue

            # Potential surnames
            if any(token.endswith(ending) for ending in surname_endings):
                if len(token) > 4:  # Reasonable length for surname
                    categories['potential_surnames'].add(token)
                continue

            # Potential first names (common Ukrainian/Russian names in top frequency)
            common_names = ['–æ–ª–µ–∫—Å–∞–Ω–¥—Ä', '–≤–æ–ª–æ–¥–∏–º–∏—Ä', '—Å–µ—Ä–≥—ñ–π', '–∞–Ω–¥—Ä—ñ–π', '–æ–ª–µ–≥', '–¥–º–∏—Ç—Ä–æ', '—è–Ω–∞', '–æ–ª—å–≥–∞', '–∞–Ω–Ω–∞', '–º–∞—Ä—ñ—è']
            if token in common_names or (len(token) >= 4 and token.endswith(('–∞', '—è', '–π', '—Ä', '–Ω'))):
                # Only if it's reasonably frequent (appears in payment descriptions)
                if frequency > 1000:
                    categories['potential_names'].add(token)
                continue

            # Everything else - potential garbage/stopwords
            # High frequency (>0.01%) non-name tokens are likely stopwords
            if percent > 0.01:
                categories['garbage_tokens'].add(token)

    return categories

def print_analysis(categories):
    """Print categorized analysis"""

    print("\n" + "="*80)
    print("üìä TOKEN ANALYSIS RESULTS")
    print("="*80)

    for category, tokens in categories.items():
        if tokens:
            print(f"\nüîñ {category.upper().replace('_', ' ')} ({len(tokens)} tokens):")
            sorted_tokens = sorted(list(tokens))

            # Print in columns for readability
            for i in range(0, len(sorted_tokens), 10):
                batch = sorted_tokens[i:i+10]
                print("   " + ", ".join(batch))

    print("\n" + "="*80)

def generate_stopwords_additions(categories):
    """Generate additions for stopwords list"""

    print("\nüéØ RECOMMENDED ADDITIONS TO STOPWORDS:")
    print("="*60)

    stopword_candidates = set()
    stopword_candidates.update(categories['payment_words'])
    stopword_candidates.update(categories['garbage_tokens'])

    # Filter out names and keep only business terms
    business_terms = set()
    for token in stopword_candidates:
        # Skip if looks like name
        if token in categories['potential_names'] or token in categories['potential_patronymics'] or token in categories['potential_surnames']:
            continue
        # Keep if business/payment related
        if len(token) >= 3:  # Reasonable length
            business_terms.add(token)

    print("\nüìù Add these to STOP_ALL in stopwords.py:")
    sorted_terms = sorted(list(business_terms))
    for i in range(0, len(sorted_terms), 8):
        batch = sorted_terms[i:i+8]
        formatted = [f'"{term}"' for term in batch]
        print("    " + ", ".join(formatted) + ",")

if __name__ == "__main__":
    categories = analyze_tokens("all_tokens_by_frequency.csv", top_n=500)
    print_analysis(categories)
    generate_stopwords_additions(categories)