#!/usr/bin/env python3
"""
Analyze remaining tokens from 1M payments to find missing stopwords
"""

import csv
import sys
sys.path.append('/Users/dariapavlova/Desktop/ai-service/src')

from ai_service.data.dicts.stopwords import STOP_ALL

def analyze_missing_stopwords(filename, top_n=2000):
    """Find tokens that should be stopwords but aren't yet"""

    print(f"üîç Analyzing top {top_n} tokens to find missing stopwords...")
    print(f"üìö Current stopwords count: {len(STOP_ALL)}")

    # Categories for new potential stopwords
    candidates = {
        'technical_terms': [],      # Technical/system terms
        'business_services': [],    # Business/service related
        'government_official': [],  # Government/official terms
        'measurement_units': [],    # Units, measurements
        'common_words': [],        # Common words that aren't names
        'unclear_abbreviations': [], # Abbreviations/codes
        'potential_garbage': []     # High frequency garbage
    }

    # Known name patterns to avoid (we don't want to remove actual names)
    name_indicators = [
        # Patronymic endings
        '–æ–≤–∏—á', '–µ–≤–∏—á', '—ñ–≤–∏—á', '–æ–≤–Ω–∞', '–µ–≤–Ω–∞', '—ñ–≤–Ω–∞', '–π–æ–≤–∏—á', '–π—ñ–≤–Ω–∞',
        # Surname endings
        '–µ–Ω–∫–æ', '—É–∫', '—é–∫', '—á—É–∫', '—Å—å–∫–∏–π', '—Å—å–∫–∞', '—Ü—å–∫–∏–π', '—Ü—å–∫–∞', '–∏—á',
        # Common name roots
        '–æ–ª–µ–∫—Å–∞–Ω–¥—Ä', '–≤–æ–ª–æ–¥–∏–º–∏—Ä', '—Å–µ—Ä–≥—ñ–π', '–∞–Ω–¥—Ä—ñ–π', '–¥–º–∏—Ç—Ä–æ', '–º–∏–∫–æ–ª–∞', '–æ–ª–µ–∫—Å—ñ–π',
        '–∞–Ω–Ω–∞', '–º–∞—Ä—ñ—è', '–æ–ª–µ–Ω–∞', '–æ–ª—å–≥–∞', '—Ç–µ—Ç—è–Ω–∞', '–Ω–∞—Ç–∞–ª—ñ—è', '—ñ—Ä–∏–Ω–∞', '—é–ª—ñ—è'
    ]

    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file)

        for i, row in enumerate(reader):
            if i >= top_n:
                break

            token = row['–¢–æ–∫–µ–Ω'].lower().strip()
            frequency = int(row['–ß–∞—Å—Ç–æ—Ç–∞'])
            percent = float(row['–ü—Ä–æ—Ü–µ–Ω—Ç'].replace('%', ''))

            # Skip if already in stopwords
            if token in STOP_ALL:
                continue

            # Skip very short tokens or numbers
            if len(token) < 3 or token.isdigit():
                continue

            # Skip if it looks like a name
            is_likely_name = False
            for indicator in name_indicators:
                if (token.endswith(indicator) or indicator in token or
                    token.startswith(indicator)):
                    is_likely_name = True
                    break

            if is_likely_name:
                continue

            # Categorize remaining tokens
            categorized = False

            # Technical/system terms
            technical_keywords = [
                '—Å–∏—Å—Ç–µ–º–∞', '—Å–µ—Ä–≤—ñ—Å', '–ø–æ—Ä—Ç–∞–ª', '–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞', '–¥–æ–¥–∞—Ç–æ–∫', '–ø—Ä–æ–≥—Ä–∞–º–∞',
                '–º–æ–±—ñ–ª—å–Ω–∏–π', '–æ–Ω–ª–∞–π–Ω', '—Ü–∏—Ñ—Ä–æ–≤', '–µ–ª–µ–∫—Ç—Ä–æ–Ω', '–∞–≤—Ç–æ–º–∞—Ç', '—Ç–µ—Ä–º—ñ–Ω–∞–ª',
                '—ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å', '–º–æ–¥—É–ª—å', '–≤–µ—Ä—Å—ñ—è', '–æ–Ω–æ–≤–ª–µ–Ω–Ω—è', '–Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è', '–∫–æ–Ω—Ñ—ñ–≥—É—Ä'
            ]

            if any(keyword in token for keyword in technical_keywords):
                candidates['technical_terms'].append((token, frequency, percent))
                categorized = True

            # Business/service terms
            if not categorized:
                business_keywords = [
                    '–±—ñ–∑–Ω–µ—Å', '–æ—Ñ—ñ—Å', '—Ü–µ–Ω—Ç—Ä', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '–≤—ñ–¥–¥—ñ–ª', '—Å–ª—É–∂–±–∞',
                    '—É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è', '–∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ü—ñ—è', '—ñ–Ω—Å–ø–µ–∫—Ü—ñ—è', '–∫–æ–º—ñ—Å—ñ—è', '–∫–æ–º—ñ—Ç–µ—Ç',
                    '—Ä–∞–¥–∏', '–∑–±–æ—Ä–∏', '–∑–∞—Å—ñ–¥–∞–Ω–Ω—è', '–Ω–∞—Ä–∞–¥–∞', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è', '—Å–µ–º—ñ–Ω–∞—Ä'
                ]

                if any(keyword in token for keyword in business_keywords):
                    candidates['business_services'].append((token, frequency, percent))
                    categorized = True

            # Government/official terms
            if not categorized:
                gov_keywords = [
                    '–¥–µ—Ä–∂–∞–≤', '–º—É–Ω—ñ—Ü–∏–ø–∞–ª', '–∫–æ–º—É–Ω–∞–ª', '–±—é–¥–∂–µ—Ç', '–∫–∞–∑–Ω–∞', '—Ñ—ñ–Ω–∞–Ω—Å',
                    '–ø–æ–¥–∞—Ç–∫', '–º–∏—Ç–æ', '–∑–±—ñ—Ä', '–≤–Ω–µ—Å–æ–∫', '–≤—ñ–¥—Ä–∞—Ö—É–≤–∞–Ω–Ω—è', '–Ω–∞—Ä–∞—Ö—É–≤–∞–Ω–Ω—è',
                    '—Ä–µ—î—Å—Ç—Ä', '–ª—ñ—Ü–µ–Ω–∑', '–¥–æ–∑–≤—ñ–ª', '—Å–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç', '–¥–æ–≤—ñ–¥–∫', '–¥–æ–≤—ñ—Ä–µ–Ω—ñ—Å—Ç—å'
                ]

                if any(keyword in token for keyword in gov_keywords):
                    candidates['government_official'].append((token, frequency, percent))
                    categorized = True

            # Measurement units and quantities
            if not categorized:
                measurement_keywords = [
                    '–º–µ—Ç—Ä', '–∫—ñ–ª–æ–º–µ—Ç—Ä', '—Å–∞–Ω—Ç–∏–º–µ—Ç—Ä', '–ª—ñ—Ç—Ä', '–∫—ñ–ª–æ–≥—Ä–∞–º', '–≥—Ä–∞–º', '—Ç–æ–Ω–Ω',
                    '—à—Ç—É–∫', '–æ–¥–∏–Ω–∏—Ü', '–µ–∫–∑–µ–º–ø–ª—è—Ä', '–∫–æ–º–ø–ª–µ–∫—Ç', '–Ω–∞–±—ñ—Ä', '–ø–∞–∫–µ—Ç', '—É–ø–∞–∫–æ–≤',
                    '–∫—É–±—ñ—á–Ω', '–∫–≤–∞–¥—Ä–∞—Ç–Ω', '–ø–æ–≥–æ–Ω–Ω', '–ª—ñ–Ω—ñ–π–Ω', '–ø–ª–æ—â', '–æ–±', '—î–º–Ω'
                ]

                if any(keyword in token for keyword in measurement_keywords):
                    candidates['measurement_units'].append((token, frequency, percent))
                    categorized = True

            # Common non-name words
            if not categorized:
                common_patterns = [
                    # High frequency (>0.1%) non-names
                    (frequency > 7000, 'potential_garbage'),
                    # Medium frequency (0.05-0.1%) descriptive words
                    (3500 <= frequency <= 7000 and len(token) >= 4, 'common_words'),
                    # Unknown abbreviations
                    (len(token) <= 5 and token.isupper(), 'unclear_abbreviations')
                ]

                for condition, category in common_patterns:
                    if condition:
                        candidates[category].append((token, frequency, percent))
                        categorized = True
                        break

            # If still not categorized but high frequency, might be garbage
            if not categorized and frequency > 1000:
                candidates['potential_garbage'].append((token, frequency, percent))

    return candidates

def print_analysis_results(candidates):
    """Print detailed analysis of potential stopwords"""

    print(f"\n" + "="*80)
    print(f"üìä MISSING STOPWORDS ANALYSIS")
    print(f"="*80)

    total_candidates = sum(len(tokens) for tokens in candidates.values())
    print(f"\nüéØ SUMMARY: Found {total_candidates} potential new stopwords")

    for category, tokens in candidates.items():
        if not tokens:
            continue

        print(f"\nüìÇ {category.upper().replace('_', ' ')} ({len(tokens)} candidates):")

        # Sort by frequency (highest first)
        sorted_tokens = sorted(tokens, key=lambda x: x[1], reverse=True)

        # Show top 15 in each category
        for i, (token, freq, percent) in enumerate(sorted_tokens[:15], 1):
            print(f"   {i:2}. {token:<20} (freq: {freq:>6}, {percent:>6.3f}%)")

        if len(sorted_tokens) > 15:
            print(f"       ... and {len(sorted_tokens) - 15} more")

    # Generate recommendations
    print(f"\nüí° RECOMMENDATIONS:")

    high_priority = []
    for category in ['potential_garbage', 'government_official', 'business_services']:
        if category in candidates and candidates[category]:
            high_priority.extend([token for token, _, _ in candidates[category][:10]])

    if high_priority:
        print(f"\nüî• HIGH PRIORITY (add immediately):")
        print(f"   {', '.join(high_priority[:20])}")
        if len(high_priority) > 20:
            print(f"   ... and {len(high_priority) - 20} more")

    print(f"\n‚ö†Ô∏è  REVIEW NEEDED:")
    print(f"   ‚Ä¢ technical_terms - might contain company names")
    print(f"   ‚Ä¢ unclear_abbreviations - could be person initials")
    print(f"   ‚Ä¢ common_words - verify they're not regional name variants")

def main():
    """Main analysis function"""

    candidates = analyze_missing_stopwords("all_tokens_by_frequency.csv", top_n=3000)
    print_analysis_results(candidates)

    # Generate code for easy addition
    print(f"\n" + "="*80)
    print("üìù CODE TO ADD TO STOPWORDS.PY:")
    print("="*80)

    all_recommendations = []
    for category, tokens in candidates.items():
        if category in ['potential_garbage', 'government_official', 'business_services', 'measurement_units']:
            all_recommendations.extend([token for token, _, _ in tokens[:5]])  # Top 5 from each category

    # Format for easy copy-paste
    print("\n# Additional stopwords from 1M payment analysis:")
    for i in range(0, len(all_recommendations), 8):
        batch = all_recommendations[i:i+8]
        formatted = [f'"{term}"' for term in batch]
        print("    " + ", ".join(formatted) + ",")

if __name__ == "__main__":
    main()