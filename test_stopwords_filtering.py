#!/usr/bin/env python3
"""
Test how our enhanced stopwords filter the payment description
"""

# Load our enhanced stopwords
import sys
sys.path.append('/Users/dariapavlova/Desktop/ai-service/src')

from ai_service.data.dicts.stopwords import STOP_ALL

def analyze_payment_text():
    """Analyze the payment text with our stopwords"""

    text = """–æ–ø–ª–∞—Ç–∞ –æ–ø–∞–ª–µ–Ω–Ω—è –æ—Ä 533481 —Ç–µ—Ä–Ω–æ–ø—ñ–ª—å –≤—É–ª–±–ª–∞–∂–∫–µ–≤–∏—á 6 –∫–≤ 190 —Å—É–º–∞ 22684 –≥—Ä–Ω –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è —Ä–∞—Ö—É–Ω–∫—É 533481 –æ–ø–ª–∞—Ç–∞ –≤–æ–¥–∞ –æ—Ä 306176 —Ç–µ—Ä–Ω–æ–ø—ñ–ª—å –≤—É–ª–±–ª–∞–∂–∫–µ–≤–∏—á 6 –∫–≤ 190 —Å—É–º–∞ 12686 –≥—Ä–Ω –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è —Ä–∞—Ö—É–Ω–∫—É 306176 –æ–ø–ª–∞—Ç–∞ –∫–≤–∞—Ä—Ç–∞–ª –ª –æ—Ä 206917 —Ç–µ—Ä–Ω–æ–ø—ñ–ª—å –≤—É–ª–±–ª–∞–∂–∫–µ–≤–∏—á 6 –∫–≤ 190 —Å—É–º–∞ 6579 –≥—Ä–Ω –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è —Ä–∞—Ö—É–Ω–∫—É 206917 –æ–ø–ª–∞—Ç–∞ —É–ø—Ä–∞–≤–±—É–¥ –æ—Ä 206917 —Ç–µ—Ä–Ω–æ–ø—ñ–ª—å –≤—É–ª–±–ª–∞–∂–∫–µ–≤–∏—á 6 –∫–≤ 190 —Å—É–º–∞ 51346 –≥—Ä–Ω –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è —Ä–∞—Ö—É–Ω–∫—É 206917 –æ–ø–ª–∞—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –≥–∞—Ä—è—á–æ–≥–æ –≤–æ–¥–æ–ø–æ—Å—Ç–∞—á–∞–Ω–Ω—è –æ—Ä 533481 —Ç–µ—Ä–Ω–æ–ø—ñ–ª—å –≤—É–ª–±–ª–∞–∂–∫–µ–≤–∏—á 6 –∫–≤ 190 —Å—É–º–∞ 6578 –≥—Ä–Ω –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è —Ä–∞—Ö—É–Ω–∫—É 533481 –æ–ø–ª–∞—Ç–∞ –≥–∞–∑ —Ä–æ–∑–ø–æ–¥—ñ–ª –æ—Ä 704254 —Ç–µ—Ä–Ω–æ–ø—ñ–ª—å –≤—É–ª–±–ª–∞–∂–∫–µ–≤–∏—á 6 –∫–≤ 190 —Å—É–º–∞ 2128 –≥—Ä–Ω –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è —Ä–∞—Ö—É–Ω–∫—É 704254 –æ–ø–ª–∞—Ç–∞ –Ω–∞—Ñ—Ç–æ–≥–∞–∑ –æ—Ä 320117936 —Ç–µ—Ä–Ω–æ–ø—ñ–ª—å –≤—É–ª–±–ª–∞–∂–∫–µ–≤–∏—á 6 –∫–≤ 190 —Å—É–º–∞ 1989 –≥—Ä–Ω –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è —Ä–∞—Ö—É–Ω–∫—É 320117936 —Å–µ–º–µ–Ω—á—É–∫ –≤—ñ–∫—Ç–æ—Ä—ñ—è —é—Ä—ñ—ó–≤–Ω–∞"""

    print("üîç TESTING STOPWORDS FILTERING")
    print("=" * 70)
    print("üìÑ Original text:")
    print(text)
    print("\n" + "=" * 70)

    # Tokenize the text (simple split for testing)
    tokens = text.lower().split()

    print(f"üìä Total tokens: {len(tokens)}")
    print(f"üìö Stopwords loaded: {len(STOP_ALL)}")

    # Filter tokens
    filtered_tokens = []
    stopword_hits = []

    for token in tokens:
        # Clean token of punctuation for better matching
        clean_token = token.strip('.,!?;:()[]{}/"\'')

        if clean_token in STOP_ALL:
            stopword_hits.append(clean_token)
        elif clean_token.isdigit():
            # Numbers should be filtered out too
            stopword_hits.append(clean_token)
        elif len(clean_token) < 3:
            # Very short tokens
            stopword_hits.append(clean_token)
        else:
            filtered_tokens.append(clean_token)

    print(f"\nüéØ FILTERING RESULTS:")
    print(f"‚úÖ Remaining tokens after filtering: {len(filtered_tokens)}")
    print(f"üö´ Filtered out (stopwords): {len(stopword_hits)}")

    print(f"\nüìù REMAINING TOKENS (potential names):")
    for i, token in enumerate(filtered_tokens, 1):
        print(f"  {i:2}. {token}")

    print(f"\nüö´ FILTERED OUT (sample of stopwords that were caught):")
    unique_stopwords = list(set(stopword_hits))[:20]  # First 20 unique
    for i, token in enumerate(unique_stopwords, 1):
        print(f"  {i:2}. {token}")
    if len(unique_stopwords) > 20:
        print(f"     ... and {len(set(stopword_hits)) - 20} more")

    print(f"\nüéØ EXPECTED RESULT:")
    print("Only names should remain: —Å–µ–º–µ–Ω—á—É–∫, –≤—ñ–∫—Ç–æ—Ä—ñ—è, —é—Ä—ñ—ó–≤–Ω–∞")

    # Check if our expected names are in the results
    expected_names = ['—Å–µ–º–µ–Ω—á—É–∫', '–≤—ñ–∫—Ç–æ—Ä—ñ—è', '—é—Ä—ñ—ó–≤–Ω–∞']
    found_names = [name for name in expected_names if name in filtered_tokens]

    print(f"\n‚úÖ FOUND EXPECTED NAMES: {found_names}")

    # Check for any unexpected remaining tokens
    unexpected = [token for token in filtered_tokens if token not in expected_names]
    if unexpected:
        print(f"‚ö†Ô∏è  UNEXPECTED REMAINING TOKENS: {unexpected}")
        print("   (These might need to be added to stopwords)")
    else:
        print("üéâ PERFECT! Only expected names remain.")

    return filtered_tokens, stopword_hits

if __name__ == "__main__":
    analyze_payment_text()