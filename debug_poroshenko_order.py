#!/usr/bin/env python3
"""
Debug why '–ü–æ—Ä–æ—à–µ–Ω–∫' is classified differently in different contexts
"""

import sys
sys.path.insert(0, '.')
sys.path.insert(0, 'src')

def debug_order_sensitivity():
    """Debug order sensitivity in classification."""
    print("üîç DEBUGGING ORDER SENSITIVITY")
    print("="*50)

    try:
        from ai_service.layers.normalization.normalization_service import NormalizationService
        from ai_service.layers.normalization.role_tagger_service import RoleTaggerService, Token, FSMState

        service = NormalizationService()

        # Test what language detection returns
        print("üåç LANGUAGE DETECTION TEST:")
        test_inputs = [
            "–ü–æ—Ä–æ—à–µ–Ω–∫ –ü–µ—Ç—Ä–æ",
            "–ü–æ—Ä–æ—à–µ–Ω–∫–æ –ü–µ—Ç—Ä–æ –û–ª–µ–∫—Å—ñ–π–æ–≤–∏—á"
        ]

        for text in test_inputs:
            from ai_service.layers.language.language_detection_service import LanguageDetectionService
            lang_service = LanguageDetectionService()
            detected = lang_service.detect_language(text)
            print(f"  '{text}' -> language: {detected['language']} (conf: {detected['confidence']:.2f})")

        # Now test FSM with detected language
        print("\nüîç FSM ROLE TAGGER TEST:")

        role_tagger = service.normalization_factory.role_tagger_service

        # Create tokens for "–ü–æ—Ä–æ—à–µ–Ω–∫ –ü–µ—Ç—Ä–æ"
        tokens = ["–ü–æ—Ä–æ—à–µ–Ω–∫", "–ü–µ—Ç—Ä–æ"]

        # Test with different languages
        for lang in ["uk", "ru", "unknown", "auto"]:
            print(f"\n  Language: {lang}")
            role_tags = role_tagger.tag(tokens, lang)

            for i, (token, tag) in enumerate(zip(tokens, role_tags)):
                print(f"    {token}: {tag.role.value} (reason: {tag.reason})")
                if tag.evidence:
                    print(f"      Evidence: {tag.evidence}")

        # Test the DefaultPersonRule directly
        print("\nüîç DEFAULT PERSON RULE TEST:")

        from ai_service.layers.normalization.role_tagger_service import DefaultPersonRule

        # Create test token for –ü–æ—Ä–æ—à–µ–Ω–∫
        test_token = Token(
            text="–ü–æ—Ä–æ—à–µ–Ω–∫",
            norm="–ü–æ—Ä–æ—à–µ–Ω–∫",
            is_capitalized=True,
            is_all_caps=False,
            has_hyphen=False,
            pos=0,
            lang="uk"
        )

        # Check what DefaultPersonRule would do at START state
        default_rule = DefaultPersonRule(role_classifier=role_tagger.role_classifier, language="uk", lexicons=role_tagger.lexicons)

        if default_rule.can_apply(FSMState.START, test_token, [test_token]):
            new_state, role, reason, evidence = default_rule.apply(FSMState.START, test_token, [test_token])
            print(f"  DefaultPersonRule at START would assign: {role.value}")
            print(f"    Reason: {reason}")
            print(f"    Evidence: {evidence}")

        # Check role classifier prediction
        if default_rule.role_classifier:
            predicted = default_rule.role_classifier._classify_personal_role("–ü–æ—Ä–æ—à–µ–Ω–∫", "uk")
            print(f"\n  Role classifier predicts: {predicted}")

    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_order_sensitivity()